{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CESNET DataZoo","text":"<p>This is the documentation of the CESNET DataZoo project. </p> <p>The goal of this project is to provide tools for working with large network traffic datasets and to facilitate research in the traffic classification area. The core functions of the <code>cesnet-datazoo</code> package are:</p> <ul> <li>A common API for downloading, configuring, and loading of three public datasets of encrypted network traffic \u2014 CESNET-TLS22, CESNET-QUIC22, and CESNET-TLS-Year22. Details about the available datasets are at the dataset overview page.</li> <li>Extensive configuration options for:<ul> <li>Selection of train, validation, and test periods. The datasets span from two weeks to one year; therefore, it is possible to evaluate classification methods in a time-based fashion that is closer to practical deployment.</li> <li>Selection of application classes and splitting classes between known and unknown. This enables research in the open-world setting, in which classification models need to handle new classes that were not seen during the training process.</li> <li>Feature scaling.</li> </ul> </li> <li>Built on suitable data structures for experiments with large datasets. There are several caching mechanisms to make repeated runs faster, for example, when searching for the best model configuration.</li> <li>Datasets are offered in multiple sizes to give users an option to start the experiments at a smaller scale (also faster dataset download, disk space, etc.). The default is the <code>S</code> size containing 25 million samples. </li> </ul>"},{"location":"dataset_metadata/","title":"DatasetMetadata","text":"<p>Each dataset class has its metadata available as a <code>DatasetMetadata</code> instance in the <code>metadata</code> attribute.</p>"},{"location":"dataset_metadata/#metadata","title":"Metadata","text":"Name CESNET-TLS22 CESNET-QUIC22 CESNET-TLS-Year22 Protocol TLS QUIC TLS Published in 2022 2023 2023 Collected in 2021 2022 2022 Collection duration 2 weeks 4 weeks 1 year Available samples 141720670 153226273 507739322 Available dataset sizes XS, S, M, L XS, S, M, L XS, S, M, L Collection period 4.10.2021 - 17.10.2021 31.10.2022 - 27.11.2022 1.1.2022 - 31.12.2022 Missing dates in collection period 20220128, 20220129, 20220130, 20221212, 20221213, 20221229, 20221230, 20221231 Application count 191 102 182 Background traffic default-background, google-background, facebook-background Features in packet sequences IPT, DIR, SIZE IPT, DIR, SIZE IPT, DIR, SIZE, PUSH_FLAG Packet histogram features PHIST_SRC_SIZES, PHIST_DST_SIZES, PHIST_SRC_IPT, PHIST_DST_IPT PHIST_SRC_SIZES, PHIST_DST_SIZES, PHIST_SRC_IPT, PHIST_DST_IPT Flowstats features BYTES, BYTES_REV, PACKETS, PACKETS_REV, PPI_LEN, PPI_ROUNDTRIPS, PPI_DURATION, DURATION BYTES, BYTES_REV, PACKETS, PACKETS_REV, PPI_LEN, PPI_ROUNDTRIPS, PPI_DURATION, DURATION, FLOW_ENDREASON_IDLE, FLOW_ENDREASON_ACTIVE, FLOW_ENDREASON_OTHER BYTES, BYTES_REV, PACKETS, PACKETS_REV, PPI_LEN, PPI_ROUNDTRIPS, PPI_DURATION, DURATION, FLOW_ENDREASON_IDLE, FLOW_ENDREASON_ACTIVE, FLOW_ENDREASON_END, FLOW_ENDREASON_OTHER TCP features FLAG_CWR, FLAG_CWR_REV, FLAG_ECE, FLAG_ECE_REV, FLAG_URG, FLAG_URG_REV, FLAG_ACK, FLAG_ACK_REV, FLAG_PSH, FLAG_PSH_REV, FLAG_RST, FLAG_RST_REV, FLAG_SYN, FLAG_SYN_REV, FLAG_FIN, FLAG_FIN_REV FLAG_CWR, FLAG_CWR_REV, FLAG_ECE, FLAG_ECE_REV, FLAG_URG, FLAG_URG_REV, FLAG_ACK, FLAG_ACK_REV, FLAG_PSH, FLAG_PSH_REV, FLAG_RST, FLAG_RST_REV, FLAG_SYN, FLAG_SYN_REV, FLAG_FIN, FLAG_FIN_REV Other fields ID ID, SRC_IP, DST_IP, DST_ASN, SRC_PORT, DST_PORT, PROTOCOL, QUIC_VERSION, QUIC_SNI, QUIC_USERAGENT, TIME_FIRST, TIME_LAST ID, SRC_IP, DST_IP, DST_ASN, DST_PORT, PROTOCOL, TLS_SNI, TLS_JA3, TIME_FIRST, TIME_LAST Cite https://doi.org/10.1016/j.comnet.2022.109467 https://doi.org/10.1016/j.dib.2023.108888 Zenodo URL https://zenodo.org/record/7965515 https://zenodo.org/record/7963302 Related papers https://doi.org/10.23919/TMA58422.2023.10199052"},{"location":"datasets_overview/","title":"Overview of datasets","text":""},{"location":"datasets_overview/#cesnet-tls22","title":"CESNET-TLS22","text":"<p>CESNET-TLS22</p> <ul> <li>TLS protocol</li> <li>Collected in 2021</li> <li>Spans two weeks</li> <li>Contains 141 million samples</li> <li>Has 191 application classes</li> </ul> <p>This dataset was published in \"Fine-grained TLS services classification with reject option\" (DOI, arXiv). It was built from live traffic collected using high-speed monitoring probes at the perimeter of the CESNET2 network.</p> <p>For detailed information about the dataset, see the linked paper and the dataset metadata page.</p>"},{"location":"datasets_overview/#cesnet-quic22","title":"CESNET-QUIC22","text":"<p>CESNET-QUIC22</p> <ul> <li>QUIC protocol</li> <li>Collected in 2022</li> <li>Spans four weeks</li> <li>Contains 153 million samples</li> <li>Has 102 application classes and three background traffic classes</li> </ul> <p>This dataset was published in \"CESNET-QUIC22: A large one-month QUIC network traffic dataset from backbone lines\" (DOI). The QUIC protocol has the potential to replace TLS over TLS as the standard protocol for reliable and secure Internet communication. Due to its design that makes the inspection of connection handshakes challenging and its usage in HTTP/3, there is an increasing demand for QUIC traffic classification methods.</p> <p>For detailed information about the dataset, see the linked paper and the dataset metadata page. Experiments based on this dataset were published in \"Encrypted traffic classification: the QUIC case\" (DOI).</p>"},{"location":"datasets_overview/#cesnet-tls-year22","title":"CESNET-TLS-Year22","text":"<p>CESNET-TLS-Year22</p> <ul> <li>TLS protocol</li> <li>Collected in 2022</li> <li>Spans one year</li> <li>Contains 507 million samples</li> <li>Has 182 application classes</li> </ul> <p>This dataset is similar to CESNET-TLS22; however, it spans the entire year 2022. It will be published in the near future.</p>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#jupyter-notebooks","title":"Jupyter notebooks","text":"<ul> <li>Explore the CESNET-QUIC22 dataset - https://nbviewer.org/github/CESNET/cesnet-datazoo/blob/main/notebooks/explore_data.ipynb</li> <li>Train a LightGBM model and evaluate it on each week of CESNET-QUIC22 - https://nbviewer.org/github/CESNET/cesnet-datazoo/blob/main/notebooks/example_evaluation.ipynb</li> </ul> <p>More example notebooks will be published in the future.</p>"},{"location":"getting_started/#code-snippets","title":"Code snippets","text":""},{"location":"getting_started/#download-a-dataset-and-compute-statistics","title":"Download a dataset and compute statistics","text":"<p><pre><code>from cesnet_datazoo.datasets import CESNET_QUIC22\ndataset = CESNET_QUIC22(\"/datasets/CESNET-QUIC22/\", size=\"XS\")\ndataset.compute_dataset_statistics(num_samples=100_000, num_workers=0)\n</code></pre> This will download the dataset, compute dataset statistics, and save them into <code>/datasets/CESNET-QUIC22/statistics</code>.</p>"},{"location":"getting_started/#enable-logging-and-set-the-spawn-method-on-windows","title":"Enable logging and set the spawn method on Windows","text":"<p><pre><code>import logging\nimport multiprocessing as mp\n\nmp.set_start_method(\"spawn\") \nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"[%(asctime)s][%(name)s][%(levelname)s] - %(message)s\")\n</code></pre> For running on Windows, we recommend using the <code>spawn</code> method for creating dataloader worker processes. Set up logging to get more information from the package.</p>"},{"location":"getting_started/#initialize-dataset-to-create-train-validation-and-test-dataframes","title":"Initialize dataset to create train, validation, and test dataframes","text":"<pre><code>from cesnet_datazoo.datasets import CESNET_QUIC22\nfrom cesnet_datazoo.config import DatasetConfig, AppSelection\n\ndataset = CESNET_QUIC22(\"/datasets/CESNET-QUIC22/\", size=\"XS\")\ndataset_config = DatasetConfig(\n    dataset=dataset,\n    apps_selection=AppSelection.ALL_KNOWN,\n    train_period=\"W-2022-44\",\n    test_period=\"W-2022-45\",\n)\ndataset.set_dataset_config_and_initialize(dataset_config)\ntrain_dataframe = dataset.get_train_df()\nval_dataframe = dataset.get_val_df()\ntest_dataframe = dataset.get_test_df()\n</code></pre> <p>The <code>DatasetConfig</code> class handles the configuration of datasets, and calling <code>set_dataset_config_and_initialize</code> initializes train, validation, and test sets with the desired configuration. Data can be read into Pandas DataFrames as shown here or via PyTorch DataLoaders. See <code>CesnetDataset</code> reference.</p>"},{"location":"installation/","title":"Installation","text":"<p>Install the package from pip with:</p> <pre><code>pip install cesnet-datazoo\n</code></pre> <p>or for editable install with:</p> <pre><code>pip install -e git+https://github.com/CESNET/cesnet-datazoo\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"Name Version tables ~=3.8.0 numpy ~=1.23.5 pandas ~=1.5.3 scikit-learn ~=1.2.0 torch ~=1.12.0 matplotlib ~=3.6.3 seaborn ~=0.12.2 tqdm ~=4.64.1 PyYAML ~=6.0 pydantic ~=1.10.4 requests ~=2.28.2"},{"location":"reference_cesnet_dataset/","title":"Base dataset class","text":""},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset","title":"datasets.cesnet_dataset.CesnetDataset","text":"<p>The main class for accessing CESNET datasets. It handles downloading, data preprocessing, train/validation/test splitting, and class selection. Access to data is provided through:</p> <ul> <li>Iterable PyTorch DataLoader for batch processing.</li> <li>Pandas DataFrame for loading the entire train, validation, or test set at once.</li> </ul> <p>The dataset is stored in a PyTables database. The internal <code>PyTablesDataset</code> class is used as a wrapper that implements the PyTorch <code>Dataset</code> interface and is compatible with <code>DataLoader</code>, which provides efficient parallel loading of the data. The dataset configuration is done through the <code>DatasetConfig</code> class.</p> <p>Intended usage:</p> <ol> <li>Create an instance of the dataset class with the desired size and data root. This will download the dataset if it has not already been downloaded.</li> <li>Create an instance of <code>DatasetConfig</code> and set it with <code>set_dataset_config_and_initialize</code>. This will initialize the dataset \u2014 select classes, split data into train/validation/test sets, and fit data scalers. All is done according to the provided configuration and is cached for later use.</li> <li>Use <code>get_train_dataloader</code> or <code>get_train_df</code> to get training data for a classification model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code> or <code>get_val_df</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code> or <code>get_test_df</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the folder where the dataset will be stored. Each dataset size has its own subfolder <code>data_root/size</code></p> required <code>size</code> <code>str</code> <p>Size of the dataset. Options are <code>XS</code>, <code>S</code>, <code>M</code>, <code>L</code>, <code>ORIG</code>.</p> <code>'S'</code> <code>silent</code> <code>bool</code> <p>Whether to suppress print and tqdm output.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the dataset.</p> <code>database_filename</code> <code>str</code> <p>Name of the database file.</p> <code>database_path</code> <code>str</code> <p>Path to the database file.</p> <code>servicemap_path</code> <code>str</code> <p>Path to the servicemap file.</p> <code>statistics_path</code> <code>str</code> <p>Path to the dataset statistics.</p> <code>bucket_url</code> <code>str</code> <p>URL of the bucket where the database is stored.</p> <code>metadata</code> <code>DatasetMetadata</code> <p>Additional dataset metadata.</p> <code>available_dates</code> <code>list[str]</code> <p>List of all available dates in the dataset.</p> <code>time_periods</code> <code>dict[str, list[str]]</code> <p>Predefined time periods. Each time period is a list of dates.</p> <code>default_train_period</code> <code>str</code> <p>Default time period for training.</p> <code>default_test_period</code> <code>str</code> <p>Default time period for testing.</p> <p>The following attributes are initialized when <code>set_dataset_config_and_initialize</code> is called.</p> <p>Attributes:</p> Name Type Description <code>dataset_config</code> <code>Optional[DatasetConfig]</code> <p>Configuration of the dataset.</p> <code>class_info</code> <code>Optional[ClassInfo]</code> <p>Structured information about the classes.</p> <code>dataset_indices</code> <code>Optional[IndicesTuple]</code> <p>Named tuple containing <code>train_indices</code>, <code>val_known_indices</code>, <code>val_unknown_indices</code>, <code>test_known_indices</code>, <code>test_unknown_indices</code>. These are the indices into PyTables database that define train, validation, and test sets.</p> <code>train_dataset</code> <code>Optional[PyTablesDataset]</code> <p>Train set in the form of <code>PyTablesDataset</code> instance wrapping the PyTables database.</p> <code>val_dataset</code> <code>Optional[PyTablesDataset]</code> <p>Validation set in the form of <code>PyTablesDataset</code> instance wrapping the PyTables database.</p> <code>test_dataset</code> <code>Optional[PyTablesDataset]</code> <p>Test set in the form of <code>PyTablesDataset</code> instance wrapping the PyTables database.</p> <code>known_apps_database_enum</code> <code>Optional[dict[int, str]]</code> <p>Dictionary that maps the database integer labels (different to those from <code>encoder</code>) of known applications to their names.</p> <code>unknown_apps_database_enum</code> <code>Optional[dict[int, str]]</code> <p>Dictionary that maps the database integer labels (different to those from <code>encoder</code>) of unknown applications to their names.</p> <code>known_app_counts</code> <code>Optional[DataFrame]</code> <p>Known application counts in the train, validation, and test sets.</p> <code>unknown_app_counts</code> <code>Optional[DataFrame]</code> <p>Unknown application counts in the validation and test sets.</p> <code>collate_fn</code> <code>Optional[Callable]</code> <p>Collate function used for creating batches in dataloaders.</p> <code>encoder</code> <code>Optional[LabelEncoder]</code> <p>Scikit-learn <code>LabelEncoder</code> used to encode class names into integers. It is fitted during the initialization of the dataset.</p> <code>flowstats_scaler</code> <code>Scaler</code> <p>Scaler for flow statistics. It is fitted during the initialization of the dataset.</p> <code>psizes_scaler</code> <code>Scaler</code> <p>Scaler for packet sizes.</p> <code>ipt_scaler</code> <code>Scaler</code> <p>Scaler for inter-packet times.</p> <code>train_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for training.</p> <code>train_dataloader_sampler</code> <code>Optional[Sampler]</code> <p>Sampler used for iterating the training dataloader. Either <code>RandomSampler</code> or <code>SequentialSampler</code>.</p> <code>val_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for validation.</p> <code>test_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for testing.</p> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>class CesnetDataset():\n    \"\"\"\n    The main class for accessing CESNET datasets. It handles downloading, data preprocessing,\n    train/validation/test splitting, and class selection. Access to data is provided through:\n\n    - Iterable PyTorch DataLoader for batch processing.\n    - Pandas DataFrame for loading the entire train, validation, or test set at once.\n\n    The dataset is stored in a [PyTables](https://www.pytables.org/) database. The internal `PyTablesDataset` class is used as a wrapper\n    that implements the PyTorch [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) interface\n    and is compatible with [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader),\n    which provides efficient parallel loading of the data. The dataset configuration is done through the [`DatasetConfig`][config.DatasetConfig] class.\n\n    **Intended usage:**\n\n    1. Create an instance of the dataset [class][dataset-classes] with the desired size and data root. This will download the dataset if it has not already been downloaded.\n    2. Create an instance of [`DatasetConfig`][config.DatasetConfig] and set it with [`set_dataset_config_and_initialize`][datasets.cesnet_dataset.CesnetDataset.set_dataset_config_and_initialize].\n    This will initialize the dataset \u2014 select classes, split data into train/validation/test sets, and fit data scalers. All is done according to the provided configuration and is cached for later use.\n    3. Use [`get_train_dataloader`][datasets.cesnet_dataset.CesnetDataset.get_train_dataloader] or [`get_train_df`][datasets.cesnet_dataset.CesnetDataset.get_train_df] to get training data for a classification model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][datasets.cesnet_dataset.CesnetDataset.get_val_dataloader] or [`get_val_df`][datasets.cesnet_dataset.CesnetDataset.get_val_df].\n    5. Evaluate the model on [`get_test_dataloader`][datasets.cesnet_dataset.CesnetDataset.get_test_dataloader] or [`get_test_df`][datasets.cesnet_dataset.CesnetDataset.get_test_df].\n\n    Parameters:\n        data_root: Path to the folder where the dataset will be stored. Each dataset size has its own subfolder `data_root/size`\n        size: Size of the dataset. Options are `XS`, `S`, `M`, `L`, `ORIG`.\n        silent: Whether to suppress print and tqdm output.\n\n    Attributes:\n        name: Name of the dataset.\n        database_filename: Name of the database file.\n        database_path: Path to the database file.\n        servicemap_path: Path to the servicemap file.\n        statistics_path: Path to the dataset statistics.\n        bucket_url: URL of the bucket where the database is stored.\n        metadata: Additional [dataset metadata][metadata].\n        available_dates: List of all available dates in the dataset.\n        time_periods: Predefined time periods. Each time period is a list of dates.\n        default_train_period: Default time period for training.\n        default_test_period: Default time period for testing.\n\n    The following attributes are initialized when [`set_dataset_config_and_initialize`][datasets.cesnet_dataset.CesnetDataset.set_dataset_config_and_initialize] is called.\n\n    Attributes:\n        dataset_config: Configuration of the dataset.\n        class_info: Structured information about the classes.\n        dataset_indices: Named tuple containing `train_indices`, `val_known_indices`, `val_unknown_indices`, `test_known_indices`, `test_unknown_indices`. These are the indices into PyTables database that define train, validation, and test sets.\n        train_dataset: Train set in the form of `PyTablesDataset` instance wrapping the PyTables database.\n        val_dataset: Validation set in the form of `PyTablesDataset` instance wrapping the PyTables database.\n        test_dataset: Test set in the form of `PyTablesDataset` instance wrapping the PyTables database.\n        known_apps_database_enum: Dictionary that maps the database integer labels (different to those from `encoder`) of known applications to their names.\n        unknown_apps_database_enum: Dictionary that maps the database integer labels (different to those from `encoder`) of unknown applications to their names.\n        known_app_counts: Known application counts in the train, validation, and test sets.\n        unknown_app_counts: Unknown application counts in the validation and test sets.\n        collate_fn: Collate function used for creating batches in dataloaders.\n        encoder: Scikit-learn [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) used to encode class names into integers. It is fitted during the initialization of the dataset.\n        flowstats_scaler: Scaler for flow statistics. It is fitted during the initialization of the dataset.\n        psizes_scaler: Scaler for packet sizes.\n        ipt_scaler: Scaler for inter-packet times.\n        train_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training.\n        train_dataloader_sampler: Sampler used for iterating the training dataloader. Either [`RandomSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler) or [`SequentialSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler).\n        val_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation.\n        test_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for testing.\n    \"\"\"\n    name: str\n    size: str\n    data_root: str\n    database_filename: str\n    database_path: str\n    servicemap_path: str\n    statistics_path: str\n    bucket_url: str\n    metadata: DatasetMetadata\n    available_dates: list[str]\n    time_periods: dict[str, list[str]]\n    default_train_period: str\n    default_test_period: str\n    time_periods_gen: bool = False\n    silent: bool = False\n\n    dataset_config: Optional[DatasetConfig] = None\n    class_info: Optional[ClassInfo] = None\n    dataset_indices: Optional[IndicesTuple] = None\n    train_dataset: Optional[PyTablesDataset] = None\n    val_dataset: Optional[PyTablesDataset] = None\n    test_dataset: Optional[PyTablesDataset] = None\n    known_apps_database_enum: Optional[dict[int, str]] = None\n    unknown_apps_database_enum: Optional[dict[int, str]] = None\n    known_app_counts: Optional[pd.DataFrame] = None\n    unknown_app_counts: Optional[pd.DataFrame] = None\n\n    collate_fn: Optional[Callable] = None\n    encoder: Optional[LabelEncoder] = None\n    flowstats_scaler: Scaler = None\n    psizes_scaler: Scaler = None\n    ipt_scaler: Scaler = None\n\n    train_dataloader: Optional[DataLoader] = None\n    train_dataloader_sampler: Optional[Sampler] = None\n    train_dataloader_drop_last: bool = True\n    val_dataloader: Optional[DataLoader] = None\n    test_dataloader: Optional[DataLoader] = None\n\n    def __init__(self, data_root: str, size: str = \"S\", skip_dataset_read_at_init: bool = False, silent: bool = False) -&gt; None:\n        self.silent = silent\n        self.metadata = load_metadata(self.name)\n        self.size = size\n        if self.size != \"ORIG\":\n            if size not in self.metadata.available_dataset_sizes:\n                raise ValueError(f\"Unknown dataset size {self.size}\")\n            self.name = f\"{self.name}-{self.size}\"\n            filename, ext = os.path.splitext(self.database_filename)\n            self.database_filename = f\"{filename}-{self.size}{ext}\"\n        self.data_root = os.path.normpath(os.path.expanduser(os.path.join(data_root, self.size)))\n        self.database_path = os.path.join(self.data_root, self.database_filename)\n        self.servicemap_path = os.path.join(self.data_root, SERVICEMAP_FILE)\n        self.statistics_path = os.path.join(self.data_root, \"statistics\")\n        if not os.path.exists(self.data_root):\n            os.makedirs(self.data_root)\n        if not self._is_downloaded():\n            self._download()\n        if not skip_dataset_read_at_init:\n            with tb.open_file(self.database_path, mode=\"r\") as database:\n                tables_paths = list(map(lambda x: x._v_pathname, iter(database.get_node(f\"/flows\"))))\n                num_samples = 0\n                for p in tables_paths:\n                    num_samples += len(database.get_node(p))\n                if self.size == \"ORIG\" and num_samples != self.metadata.available_samples:\n                    raise ValueError(f\"Expected {self.metadata.available_samples} samples, but got {num_samples} in the database. Please delete the data root folder, update cesnet-datazoo, and redownload the dataset.\")\n                elif num_samples != DATASET_SIZES[self.size]:\n                    raise ValueError(f\"Expected {DATASET_SIZES[self.size]} samples, but got {num_samples} in the database. Please delete the data root folder, update cesnet-datazoo, and redownload the dataset.\")\n                self.available_dates = list(map(lambda x: x.removeprefix(\"/flows/D\"), tables_paths))\n        else:\n            self.available_dates = []\n        if self.time_periods_gen:\n            self._generate_time_periods()\n\n    def set_dataset_config_and_initialize(self, dataset_config: DatasetConfig) -&gt; None:\n        \"\"\"\n        Initialize train, validation, and test sets. Data cannot be accessed before calling this method.\n\n        Parameters:\n            dataset_config: Desired configuration of the dataset.\n        \"\"\"\n        self.dataset_config = dataset_config\n        self._clear()\n        self._initialize_train_val_test()\n\n    def get_train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Provides a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training. The dataloader is created on the first call and then cached.\n        When the dataloader is iterated in random order, the last incomplete batch is dropped.\n        The dataloader is configured with the following config attributes:\n\n        | Dataset config               | Description                                                                                |\n        | ---------------------------- | ------------------------------------------------------------------------------------------ |\n        | `batch_size`                 | Number of samples per batch.                                                               |\n        | `train_workers`              | Number of workers for loading train data.                                                  |\n        | `train_dataloader_order`     | Whether to load train data in sequential or random order. See [config.DataLoaderOrder][].  |\n        | `train_dataloader_seed`      | Seed for loading train data in random order.                                               |\n\n        Returns:\n            Train data as an iterable dataloader.\n        \"\"\"\n        if self.dataset_config is None:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting train dataloader\")\n        assert self.train_dataset\n        if self.train_dataloader:\n            return self.train_dataloader\n        # Create sampler according to the selected order\n        if self.dataset_config.train_dataloader_order == DataLoaderOrder.RANDOM:\n            if self.dataset_config.train_dataloader_seed is not None:\n                generator = torch.Generator()\n                generator.manual_seed(self.dataset_config.train_dataloader_seed)\n            else:\n                generator = None\n            self.train_dataloader_sampler = RandomSampler(self.train_dataset, generator=generator)\n            self.train_dataloader_drop_last = True\n        elif self.dataset_config.train_dataloader_order == DataLoaderOrder.SEQUENTIAL:\n            self.train_dataloader_sampler = SequentialSampler(self.train_dataset)\n            self.train_dataloader_drop_last = False\n        else: assert_never(self.dataset_config.train_dataloader_order)\n        # Create dataloader\n        batch_sampler = BatchSampler(sampler=self.train_dataloader_sampler, batch_size=self.dataset_config.batch_size, drop_last=self.train_dataloader_drop_last)\n        train_dataloader = DataLoader(\n            self.train_dataset,\n            num_workers=self.dataset_config.train_workers,\n            worker_init_fn=worker_init_fn,\n            collate_fn=self.collate_fn,\n            persistent_workers=self.dataset_config.train_workers &gt; 0,\n            batch_size=None,\n            sampler=batch_sampler,)\n        if self.dataset_config.train_workers == 0:\n            self.train_dataset.pytables_worker_init()\n        self.train_dataloader = train_dataloader\n        return train_dataloader\n\n    def get_val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Provides a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation.\n        The dataloader is created on the first call and then cached.\n        The dataloader is configured with the following config attributes:\n\n        | Dataset config    | Description                                                       |\n        | ------------------| ------------------------------------------------------------------|\n        | `test_batch_size` | Number of samples per batch for loading validation and test data. |\n        | `val_workers`     | Number of workers for loading validation data.                    |\n\n        Returns:\n            Validation data as an iterable dataloader.\n        \"\"\"\n        if self.dataset_config is None:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting validaion dataloader\")\n        assert self.val_dataset is not None\n        if self.dataset_config.val_approach == ValidationApproach.NO_VALIDATION:\n            raise ValueError(\"Validation dataloader is not available when using no-validation\")\n        if self.val_dataloader:\n            return self.val_dataloader\n        batch_sampler = BatchSampler(sampler=SequentialSampler(self.val_dataset), batch_size=self.dataset_config.test_batch_size, drop_last=False)\n        val_dataloader = DataLoader(\n            self.val_dataset,\n            num_workers=self.dataset_config.val_workers,\n            worker_init_fn=worker_init_fn,\n            collate_fn=self.collate_fn,\n            persistent_workers=self.dataset_config.val_workers &gt; 0,\n            batch_size=None,\n            sampler=batch_sampler,)\n        if self.dataset_config.val_workers == 0:\n            self.val_dataset.pytables_worker_init()\n        self.val_dataloader = val_dataloader\n        return val_dataloader\n\n    def get_test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Provides a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for testing.\n        The dataloader is created on the first call and then cached.\n\n        When the dataset is used in the open-world setting, and unknown classes are defined,\n        the test dataloader returns `test_known_size` samples of known classes followed by `test_unknown_size` samples of unknown classes.\n\n        The dataloader is configured with the following config attributes:\n\n        | Dataset config    | Description                                                       |\n        | ------------------| ------------------------------------------------------------------|\n        | `test_batch_size` | Number of samples per batch for loading validation and test data. |\n        | `test_workers`    | Number of workers for loading test data.                          |\n\n        Returns:\n            Test data as an iterable dataloader.\n        \"\"\"\n        if self.dataset_config is None:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting test dataloader\")\n        assert self.test_dataset is not None\n        if self.test_dataloader:\n            return self.test_dataloader\n        batch_sampler = BatchSampler(sampler=SequentialSampler(self.test_dataset), batch_size=self.dataset_config.test_batch_size, drop_last=False)\n        test_dataloader = DataLoader(\n            self.test_dataset,\n            num_workers=self.dataset_config.test_workers,\n            worker_init_fn=worker_init_fn,\n            collate_fn=self.collate_fn,\n            persistent_workers=False,\n            batch_size=None,\n            sampler=batch_sampler,)\n        if self.dataset_config.test_workers == 0:\n            self.test_dataset.pytables_worker_init()\n        self.test_dataloader = test_dataloader\n        return test_dataloader\n\n    def get_dataloaders(self) -&gt; tuple[DataLoader, DataLoader, DataLoader]:\n        \"\"\"Gets train, validation, and test dataloaders in one call.\"\"\"\n        if self.dataset_config is None:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting dataloaders\")\n        train_dataloader = self.get_train_dataloader()\n        val_dataloader = self.get_val_dataloader()\n        test_dataloader = self.get_test_dataloader()\n        return train_dataloader, val_dataloader, test_dataloader\n\n    def get_train_df(self, flatten_ppi: bool = False) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a train Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The dataframe is in sequential (datetime) order. Consider shuffling the dataframe if needed.\n\n        !!! warning \"Memory usage\"\n\n            The whole train set is loaded into memory. If the dataset size is larger than `'S'`, consider using `get_train_dataloader` instead.\n\n        Parameters:\n            flatten_ppi: Whether to flatten the PPI sequence into individual columns (named `IPT_X`, `DIR_X`, `SIZE_X`, `PUSH_X`, *X* being the index of the packet) or keep one `PPI` column with 2D data.\n\n        Returns:\n            Train data as a dataframe.\n        \"\"\"\n        self._check_before_dataframe()\n        assert self.dataset_config is not None and self.train_dataset is not None\n        if len(self.train_dataset) &gt; DATAFRAME_SAMPLES_WARNING_THRESHOLD:\n            warnings.warn(f\"Train set has ({len(self.train_dataset)} samples), consider using get_train_dataloader() instead\")\n        train_dataloader = self.get_train_dataloader()\n        assert isinstance(train_dataloader.sampler, BatchSampler) and self.train_dataloader_sampler is not None\n        # Read dataloader in sequential order\n        train_dataloader.sampler.sampler = SequentialSampler(self.train_dataset)\n        train_dataloader.sampler.drop_last = False\n        feature_names = self.dataset_config.get_feature_names(flatten_ppi=flatten_ppi)\n        df = create_df_from_dataloader(dataloader=train_dataloader, feature_names=feature_names, flatten_ppi=flatten_ppi, silent=self.silent)\n        # Restore the original dataloader sampler and drop_last\n        train_dataloader.sampler.sampler = self.train_dataloader_sampler\n        train_dataloader.sampler.drop_last = self.train_dataloader_drop_last\n        return df\n\n    def get_val_df(self, flatten_ppi: bool = False) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates validation Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The dataframe is in sequential (datetime) order.\n\n        !!! warning \"Memory usage\"\n\n            The whole validation set is loaded into memory. If the dataset size is larger than `'S'`, consider using `get_val_dataloader` instead.\n\n        Parameters:\n            flatten_ppi: Whether to flatten the PPI sequence into individual columns (named `IPT_X`, `DIR_X`, `SIZE_X`, `PUSH_X`, *X* being the index of the packet) or keep one `PPI` column with 2D data.\n\n        Returns:\n            Validation data as a dataframe.\n        \"\"\"\n        self._check_before_dataframe()\n        assert self.dataset_config is not None and self.val_dataset is not None\n        if len(self.val_dataset) &gt; DATAFRAME_SAMPLES_WARNING_THRESHOLD:\n            warnings.warn(f\"Validation set has ({len(self.val_dataset)} samples), consider using get_val_dataloader() instead\")\n        feature_names = self.dataset_config.get_feature_names(flatten_ppi=flatten_ppi)\n        return create_df_from_dataloader(dataloader=self.get_val_dataloader(), feature_names=feature_names, flatten_ppi=flatten_ppi, silent=self.silent)\n\n    def get_test_df(self, flatten_ppi: bool = False) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates test Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The dataframe is in sequential (datetime) order.\n\n\n        When the dataset is used in the open-world setting, and unknown classes are defined,\n        the returned test dataframe is composed of `test_known_size` samples of known classes followed by `test_unknown_size` samples of unknown classes.\n\n\n        !!! warning \"Memory usage\"\n\n            The whole test set is loaded into memory. If the dataset size is larger than `'S'`, consider using `get_test_dataloader` instead.\n\n        Parameters:\n            flatten_ppi: Whether to flatten the PPI sequence into individual columns (named `IPT_X`, `DIR_X`, `SIZE_X`, `PUSH_X`, *X* being the index of the packet) or keep one `PPI` column with 2D data.\n\n        Returns:\n            Test data as a dataframe.\n        \"\"\"\n        self._check_before_dataframe()\n        assert self.dataset_config is not None and self.test_dataset is not None\n        if len(self.test_dataset) &gt; DATAFRAME_SAMPLES_WARNING_THRESHOLD:\n            warnings.warn(f\"Test set has ({len(self.test_dataset)} samples), consider using get_test_dataloader() instead\")\n        feature_names = self.dataset_config.get_feature_names(flatten_ppi=flatten_ppi)\n        return create_df_from_dataloader(dataloader=self.get_test_dataloader(), feature_names=feature_names, flatten_ppi=flatten_ppi, silent=self.silent)\n\n    def compute_dataset_statistics(self, num_samples: int | Literal[\"all\"] = 10_000_000, num_workers: int = 4, batch_size: int = 4096, disabled_apps: Optional[list[str]] = None)-&gt; None:\n        \"\"\"\n        Computes dataset statistics and saves them to the `statistics_path` folder.\n\n        Parameters:\n            num_samples: Number of samples to use for computing the statistics.\n            num_workers: Number of workers for loading data.\n            batch_size: Number of samples per batch for loading data.\n            disabled_apps: List of applications to exclude from the statistics.\n        \"\"\"\n        if self.name.startswith(\"CESNET-TLS22\"):\n            raise NotImplementedError(\"Dataset statistics are not supported for CESNET_TLS22\")\n        flowstats_features = self.metadata.flowstats_features + self.metadata.packet_histogram_features + self.metadata.tcp_features\n        if not os.path.exists(self.statistics_path):\n            os.mkdir(self.statistics_path)\n        compute_dataset_statistics(database_path=self.database_path,\n                                   output_dir=self.statistics_path,\n                                   flowstats_features=flowstats_features,\n                                   protocol=self.metadata.protocol,\n                                   disabled_apps=disabled_apps if disabled_apps is not None else [],\n                                   num_samples=num_samples,\n                                   num_workers=num_workers,\n                                   batch_size=batch_size,\n                                   silent=self.silent)\n\n    def _generate_time_periods(self) -&gt; None:\n        time_periods = {}\n        for period in self.time_periods:\n            time_periods[period] = []\n            if period.startswith(\"W\"):\n                split = period.split(\"-\")\n                collection_year, week = int(split[1]), int(split[2])\n                for d in range(1, 8):\n                    s = datetime.date.fromisocalendar(collection_year, week, d).strftime(\"%Y%m%d\")\n                    if s not in self.metadata.missing_dates_in_collection_period:\n                        time_periods[period].append(s)\n            if period.startswith(\"M\"):\n                split = period.split(\"-\")\n                collection_year, month = int(split[1]), int(split[2])\n                for d in range(1, calendar.monthrange(collection_year, month)[1]):\n                    s = datetime.date(collection_year, month, d).strftime(\"%Y%m%d\")\n                    if s not in self.metadata.missing_dates_in_collection_period:\n                        time_periods[period].append(s)\n        self.time_periods = time_periods\n\n    def _is_downloaded(self) -&gt; bool:\n        \"\"\"Servicemap is downloaded after the database; thus if it exists, the database is also downloaded\"\"\"\n        return os.path.exists(self.servicemap_path) and os.path.exists(self.database_path)\n\n    def _download(self) -&gt; None:\n        if not self.silent:\n            print(f\"Downloading {self.name} dataset\")\n        database_url = f\"{self.bucket_url}&amp;file={self.database_filename}\"\n        servicemap_url = f\"{self.bucket_url}&amp;file={SERVICEMAP_FILE}\"\n        resumable_download(url=database_url, file_path=self.database_path, silent=self.silent)\n        simple_download(url=servicemap_url, file_path=self.servicemap_path)\n\n    def _clear(self) -&gt; None:\n        self.class_info = None\n        self.dataset_indices = None\n        self.train_dataset = None\n        self.val_dataset = None\n        self.test_dataset = None\n        self.known_apps_database_enum = None\n        self.unknown_apps_database_enum = None\n        self.known_app_counts = None\n        self.unknown_app_counts = None\n\n        self.collate_fn = None\n        self.encoder = None\n        self.flowstats_scaler = None\n        self.psizes_scaler = None\n        self.ipt_scaler = None\n\n        self.train_dataloader = None\n        self.train_dataloader_sampler = None\n        self.train_dataloader_drop_last = True\n        self.val_dataloader = None\n        self.test_dataloader = None\n\n    def _check_before_dataframe(self) -&gt; None:\n        if self.dataset_config is None:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting a dataframe\")\n        if self.dataset_config.return_ips:\n            raise ValueError(\"Dataframes are not available when return_ips is set. Use a dataloader instead.\")\n        if self.dataset_config.return_torch:\n            raise ValueError(\"Dataframes are not available when return_torch is set. Use a dataloader instead.\")\n\n    def _initialize_train_val_test(self) -&gt; None:\n        assert self.dataset_config is not None\n        dataset_config = self.dataset_config\n        servicemap = pd.read_csv(dataset_config.servicemap_path, index_col=\"Tag\")\n        # Initialize train and test indices\n        train_indices, train_unknown_indices, encoder, known_apps_database_enum, unknown_apps_database_enum = init_or_load_train_indices(dataset_config=dataset_config, servicemap=servicemap)\n        test_known_indices, test_unknown_indices, test_data_path = init_or_load_test_indices(dataset_config=dataset_config, known_apps_database_enum=known_apps_database_enum, unknown_apps_database_enum=unknown_apps_database_enum)\n        # Date weight sampling of train indices\n        if dataset_config.train_dates_weigths is not None:\n            assert dataset_config.train_size != \"all\"\n            if dataset_config.val_approach == ValidationApproach.SPLIT_FROM_TRAIN:\n                # requested number of samples is train_size + val_known_size when using the split-from-train validation approach\n                assert dataset_config.val_known_size != \"all\"\n                num_samples = dataset_config.train_size + dataset_config.val_known_size\n            else:\n                num_samples = dataset_config.train_size\n            if num_samples &gt; len(train_indices):\n                raise ValueError(f\"Requested number of samples for weight sampling ({num_samples}) is larger than the number of available train samples ({len(train_indices)})\")\n            train_indices = date_weight_sample_train_indices(dataset_config=dataset_config, train_indices=train_indices, num_samples=num_samples)\n        # Obtain validation indices based on the selected approach\n        if dataset_config.val_approach == ValidationApproach.VALIDATION_DATES:\n            val_known_indices, val_unknown_indices, val_data_path = init_or_load_val_indices(dataset_config=dataset_config, known_apps_database_enum=known_apps_database_enum, unknown_apps_database_enum=unknown_apps_database_enum)\n        elif dataset_config.val_approach == ValidationApproach.SPLIT_FROM_TRAIN:\n            train_val_rng = get_fresh_random_generator(dataset_config=dataset_config, section=RandomizedSection.TRAIN_VAL_SPLIT)\n            val_data_path = dataset_config._get_train_data_path()\n            val_unknown_indices = train_unknown_indices\n            train_labels = train_indices[:, INDICES_LABEL_POS]\n            if dataset_config.train_dates_weigths is not None:\n                assert dataset_config.val_known_size != \"all\"\n                # When weight sampling is used, val_known_size is kept but the resulting train size can be smaller due to no enough samples in some train dates\n                if dataset_config.val_known_size &gt; len(train_indices):\n                    raise ValueError(f\"Requested validation size ({dataset_config.val_known_size}) is larger than the number of available train samples after weight sampling ({len(train_indices)})\")\n                train_indices, val_known_indices = train_test_split(train_indices, test_size=dataset_config.val_known_size, stratify=train_labels, shuffle=True, random_state=train_val_rng)\n                dataset_config.train_size = len(train_indices)\n            elif dataset_config.train_size == \"all\" and dataset_config.val_known_size == \"all\":\n                train_indices, val_known_indices = train_test_split(train_indices, test_size=dataset_config.train_val_split_fraction, stratify=train_labels, shuffle=True, random_state=train_val_rng)\n            else:\n                if dataset_config.val_known_size != \"all\" and  dataset_config.train_size != \"all\" and dataset_config.train_size + dataset_config.val_known_size &gt; len(train_indices):\n                    raise ValueError(f\"Requested train size + validation size ({dataset_config.train_size + dataset_config.val_known_size}) is larger than the number of available train samples ({len(train_indices)})\")\n                if dataset_config.train_size != \"all\" and dataset_config.train_size &gt; len(train_indices):\n                    raise ValueError(f\"Requested train size ({dataset_config.train_size}) is larger than the number of available train samples ({len(train_indices)})\")\n                if dataset_config.val_known_size != \"all\" and dataset_config.val_known_size &gt; len(train_indices):\n                    raise ValueError(f\"Requested validation size ({dataset_config.val_known_size}) is larger than the number of available train samples ({len(train_indices)})\")\n                train_indices, val_known_indices = train_test_split(train_indices,\n                                                                          train_size=dataset_config.train_size if dataset_config.train_size != \"all\" else None,\n                                                                          test_size=dataset_config.val_known_size if dataset_config.val_known_size != \"all\" else None,\n                                                                          stratify=train_labels, shuffle=True, random_state=train_val_rng)\n        elif dataset_config.val_approach == ValidationApproach.NO_VALIDATION:\n            val_known_indices = val_unknown_indices = val_data_path = None\n        else: assert_never(dataset_config.val_approach)\n\n        # Create class info\n        class_info = create_superclass_structures(servicemap=servicemap, target_names=list(encoder.classes_))\n        # Load or fit data scalers\n        flowstats_scaler, flowstats_quantiles, ipt_scaler, psizes_scaler = fit_or_load_scalers(dataset_config=dataset_config, train_indices=train_indices)\n        # Subset dataset indices based on the selected sizes and compute application counts\n        dataset_indices = IndicesTuple(train_indices=train_indices, val_known_indices=val_known_indices, val_unknown_indices=val_unknown_indices, test_known_indices=test_known_indices, test_unknown_indices=test_unknown_indices)\n        dataset_indices = subset_and_sort_indices(dataset_config=dataset_config, dataset_indices=dataset_indices)\n        known_app_counts = compute_known_app_counts(dataset_indices=dataset_indices, database_enum=known_apps_database_enum)\n        unknown_app_counts = compute_unknown_app_counts(dataset_indices=dataset_indices, database_enum=unknown_apps_database_enum)\n        # Combine known and unknown test indicies to create a single dataloader\n        assert isinstance(dataset_config.test_unknown_size, int)\n        if dataset_config.test_unknown_size &gt; 0 and len(unknown_apps_database_enum) &gt; 0:\n            test_combined_indices = np.concatenate((dataset_indices.test_known_indices, dataset_indices.test_unknown_indices))\n        else:\n            test_combined_indices = dataset_indices.test_known_indices\n\n        # Create train, validation, and test datasets\n        train_dataset = PyTablesDataset(\n            database_path=dataset_config.database_path,\n            tables_paths=dataset_config._get_train_tables_paths(),\n            indices=dataset_indices.train_indices,\n            flowstats_features=dataset_config.flowstats_features,\n            return_ips=dataset_config.return_ips,)\n        test_dataset = PyTablesDataset(\n            database_path=dataset_config.database_path,\n            tables_paths=dataset_config._get_test_tables_paths(),\n            indices=test_combined_indices,\n            flowstats_features=dataset_config.flowstats_features,\n            preload=dataset_config.preload_test,\n            preload_blob=os.path.join(test_data_path, \"preload\", f\"test_dataset-{dataset_config.test_known_size}-{dataset_config.test_unknown_size}.npz\"),\n            return_ips=dataset_config.return_ips,)\n        val_dataset = None\n        if dataset_config.val_approach != ValidationApproach.NO_VALIDATION:\n            assert val_data_path is not None\n            val_dataset = PyTablesDataset(\n            database_path=dataset_config.database_path,\n            tables_paths=dataset_config._get_train_tables_paths(),\n            indices=dataset_indices.val_known_indices,\n            flowstats_features=dataset_config.flowstats_features,\n            preload=True,\n            preload_blob=os.path.join(val_data_path, \"preload\", f\"val_dataset-{dataset_config.val_known_size}.npz\"),\n            return_ips=dataset_config.return_ips,)\n        # Create collate function\n        if dataset_config.return_ips:\n            collate_fn = pytables_ip_collate_fn\n        else:\n            collate_fn = partial(pytables_collate_fn, # type: ignore\n                use_packet_histograms=dataset_config.use_packet_histograms,\n                flowstats_scaler=flowstats_scaler,\n                flowstats_quantiles=flowstats_quantiles,\n                psizes_scaler=psizes_scaler,\n                psizes_max=dataset_config.psizes_max,\n                ipt_scaler=ipt_scaler,\n                ipt_min=dataset_config.ipt_min,\n                ipt_max=dataset_config.ipt_max,\n                use_push_flags=dataset_config.use_push_flags,\n                zero_ppi_start=dataset_config.zero_ppi_start,\n                encoder=encoder,\n                known_apps=class_info.known_apps,\n                return_torch=dataset_config.return_torch,)\n        self.class_info = class_info\n        self.dataset_indices = dataset_indices\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.test_dataset = test_dataset\n        self.known_apps_database_enum = known_apps_database_enum\n        self.unknown_apps_database_enum = unknown_apps_database_enum\n        self.known_app_counts = known_app_counts\n        self.unknown_app_counts = unknown_app_counts\n        self.collate_fn = collate_fn\n        self.encoder = encoder\n        self.flowstats_scaler = flowstats_scaler\n        self.psizes_scaler = psizes_scaler\n        self.ipt_scaler = ipt_scaler\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.set_dataset_config_and_initialize","title":"set_dataset_config_and_initialize","text":"<pre><code>set_dataset_config_and_initialize(\n    dataset_config: DatasetConfig,\n) -&gt; None\n</code></pre> <p>Initialize train, validation, and test sets. Data cannot be accessed before calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_config</code> <code>DatasetConfig</code> <p>Desired configuration of the dataset.</p> required Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_dataset_config_and_initialize(self, dataset_config: DatasetConfig) -&gt; None:\n    \"\"\"\n    Initialize train, validation, and test sets. Data cannot be accessed before calling this method.\n\n    Parameters:\n        dataset_config: Desired configuration of the dataset.\n    \"\"\"\n    self.dataset_config = dataset_config\n    self._clear()\n    self._initialize_train_val_test()\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.get_train_dataloader","title":"get_train_dataloader","text":"<pre><code>get_train_dataloader() -&gt; DataLoader\n</code></pre> <p>Provides a PyTorch <code>DataLoader</code> for training. The dataloader is created on the first call and then cached. When the dataloader is iterated in random order, the last incomplete batch is dropped. The dataloader is configured with the following config attributes:</p> Dataset config Description <code>batch_size</code> Number of samples per batch. <code>train_workers</code> Number of workers for loading train data. <code>train_dataloader_order</code> Whether to load train data in sequential or random order. See config.DataLoaderOrder. <code>train_dataloader_seed</code> Seed for loading train data in random order. <p>Returns:</p> Type Description <code>DataLoader</code> <p>Train data as an iterable dataloader.</p> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Provides a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training. The dataloader is created on the first call and then cached.\n    When the dataloader is iterated in random order, the last incomplete batch is dropped.\n    The dataloader is configured with the following config attributes:\n\n    | Dataset config               | Description                                                                                |\n    | ---------------------------- | ------------------------------------------------------------------------------------------ |\n    | `batch_size`                 | Number of samples per batch.                                                               |\n    | `train_workers`              | Number of workers for loading train data.                                                  |\n    | `train_dataloader_order`     | Whether to load train data in sequential or random order. See [config.DataLoaderOrder][].  |\n    | `train_dataloader_seed`      | Seed for loading train data in random order.                                               |\n\n    Returns:\n        Train data as an iterable dataloader.\n    \"\"\"\n    if self.dataset_config is None:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting train dataloader\")\n    assert self.train_dataset\n    if self.train_dataloader:\n        return self.train_dataloader\n    # Create sampler according to the selected order\n    if self.dataset_config.train_dataloader_order == DataLoaderOrder.RANDOM:\n        if self.dataset_config.train_dataloader_seed is not None:\n            generator = torch.Generator()\n            generator.manual_seed(self.dataset_config.train_dataloader_seed)\n        else:\n            generator = None\n        self.train_dataloader_sampler = RandomSampler(self.train_dataset, generator=generator)\n        self.train_dataloader_drop_last = True\n    elif self.dataset_config.train_dataloader_order == DataLoaderOrder.SEQUENTIAL:\n        self.train_dataloader_sampler = SequentialSampler(self.train_dataset)\n        self.train_dataloader_drop_last = False\n    else: assert_never(self.dataset_config.train_dataloader_order)\n    # Create dataloader\n    batch_sampler = BatchSampler(sampler=self.train_dataloader_sampler, batch_size=self.dataset_config.batch_size, drop_last=self.train_dataloader_drop_last)\n    train_dataloader = DataLoader(\n        self.train_dataset,\n        num_workers=self.dataset_config.train_workers,\n        worker_init_fn=worker_init_fn,\n        collate_fn=self.collate_fn,\n        persistent_workers=self.dataset_config.train_workers &gt; 0,\n        batch_size=None,\n        sampler=batch_sampler,)\n    if self.dataset_config.train_workers == 0:\n        self.train_dataset.pytables_worker_init()\n    self.train_dataloader = train_dataloader\n    return train_dataloader\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.get_val_dataloader","title":"get_val_dataloader","text":"<pre><code>get_val_dataloader() -&gt; DataLoader\n</code></pre> <p>Provides a PyTorch <code>DataLoader</code> for validation. The dataloader is created on the first call and then cached. The dataloader is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of samples per batch for loading validation and test data. <code>val_workers</code> Number of workers for loading validation data. <p>Returns:</p> Type Description <code>DataLoader</code> <p>Validation data as an iterable dataloader.</p> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Provides a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation.\n    The dataloader is created on the first call and then cached.\n    The dataloader is configured with the following config attributes:\n\n    | Dataset config    | Description                                                       |\n    | ------------------| ------------------------------------------------------------------|\n    | `test_batch_size` | Number of samples per batch for loading validation and test data. |\n    | `val_workers`     | Number of workers for loading validation data.                    |\n\n    Returns:\n        Validation data as an iterable dataloader.\n    \"\"\"\n    if self.dataset_config is None:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting validaion dataloader\")\n    assert self.val_dataset is not None\n    if self.dataset_config.val_approach == ValidationApproach.NO_VALIDATION:\n        raise ValueError(\"Validation dataloader is not available when using no-validation\")\n    if self.val_dataloader:\n        return self.val_dataloader\n    batch_sampler = BatchSampler(sampler=SequentialSampler(self.val_dataset), batch_size=self.dataset_config.test_batch_size, drop_last=False)\n    val_dataloader = DataLoader(\n        self.val_dataset,\n        num_workers=self.dataset_config.val_workers,\n        worker_init_fn=worker_init_fn,\n        collate_fn=self.collate_fn,\n        persistent_workers=self.dataset_config.val_workers &gt; 0,\n        batch_size=None,\n        sampler=batch_sampler,)\n    if self.dataset_config.val_workers == 0:\n        self.val_dataset.pytables_worker_init()\n    self.val_dataloader = val_dataloader\n    return val_dataloader\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.get_test_dataloader","title":"get_test_dataloader","text":"<pre><code>get_test_dataloader() -&gt; DataLoader\n</code></pre> <p>Provides a PyTorch <code>DataLoader</code> for testing. The dataloader is created on the first call and then cached.</p> <p>When the dataset is used in the open-world setting, and unknown classes are defined, the test dataloader returns <code>test_known_size</code> samples of known classes followed by <code>test_unknown_size</code> samples of unknown classes.</p> <p>The dataloader is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of samples per batch for loading validation and test data. <code>test_workers</code> Number of workers for loading test data. <p>Returns:</p> Type Description <code>DataLoader</code> <p>Test data as an iterable dataloader.</p> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Provides a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for testing.\n    The dataloader is created on the first call and then cached.\n\n    When the dataset is used in the open-world setting, and unknown classes are defined,\n    the test dataloader returns `test_known_size` samples of known classes followed by `test_unknown_size` samples of unknown classes.\n\n    The dataloader is configured with the following config attributes:\n\n    | Dataset config    | Description                                                       |\n    | ------------------| ------------------------------------------------------------------|\n    | `test_batch_size` | Number of samples per batch for loading validation and test data. |\n    | `test_workers`    | Number of workers for loading test data.                          |\n\n    Returns:\n        Test data as an iterable dataloader.\n    \"\"\"\n    if self.dataset_config is None:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting test dataloader\")\n    assert self.test_dataset is not None\n    if self.test_dataloader:\n        return self.test_dataloader\n    batch_sampler = BatchSampler(sampler=SequentialSampler(self.test_dataset), batch_size=self.dataset_config.test_batch_size, drop_last=False)\n    test_dataloader = DataLoader(\n        self.test_dataset,\n        num_workers=self.dataset_config.test_workers,\n        worker_init_fn=worker_init_fn,\n        collate_fn=self.collate_fn,\n        persistent_workers=False,\n        batch_size=None,\n        sampler=batch_sampler,)\n    if self.dataset_config.test_workers == 0:\n        self.test_dataset.pytables_worker_init()\n    self.test_dataloader = test_dataloader\n    return test_dataloader\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.get_dataloaders","title":"get_dataloaders","text":"<pre><code>get_dataloaders() -&gt; (\n    tuple[DataLoader, DataLoader, DataLoader]\n)\n</code></pre> <p>Gets train, validation, and test dataloaders in one call.</p> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_dataloaders(self) -&gt; tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"Gets train, validation, and test dataloaders in one call.\"\"\"\n    if self.dataset_config is None:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting dataloaders\")\n    train_dataloader = self.get_train_dataloader()\n    val_dataloader = self.get_val_dataloader()\n    test_dataloader = self.get_test_dataloader()\n    return train_dataloader, val_dataloader, test_dataloader\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.get_train_df","title":"get_train_df","text":"<pre><code>get_train_df(flatten_ppi: bool = False) -&gt; pd.DataFrame\n</code></pre> <p>Creates a train Pandas <code>DataFrame</code>. The dataframe is in sequential (datetime) order. Consider shuffling the dataframe if needed.</p> <p>Memory usage</p> <p>The whole train set is loaded into memory. If the dataset size is larger than <code>'S'</code>, consider using <code>get_train_dataloader</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>flatten_ppi</code> <code>bool</code> <p>Whether to flatten the PPI sequence into individual columns (named <code>IPT_X</code>, <code>DIR_X</code>, <code>SIZE_X</code>, <code>PUSH_X</code>, X being the index of the packet) or keep one <code>PPI</code> column with 2D data.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Train data as a dataframe.</p> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_df(self, flatten_ppi: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a train Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The dataframe is in sequential (datetime) order. Consider shuffling the dataframe if needed.\n\n    !!! warning \"Memory usage\"\n\n        The whole train set is loaded into memory. If the dataset size is larger than `'S'`, consider using `get_train_dataloader` instead.\n\n    Parameters:\n        flatten_ppi: Whether to flatten the PPI sequence into individual columns (named `IPT_X`, `DIR_X`, `SIZE_X`, `PUSH_X`, *X* being the index of the packet) or keep one `PPI` column with 2D data.\n\n    Returns:\n        Train data as a dataframe.\n    \"\"\"\n    self._check_before_dataframe()\n    assert self.dataset_config is not None and self.train_dataset is not None\n    if len(self.train_dataset) &gt; DATAFRAME_SAMPLES_WARNING_THRESHOLD:\n        warnings.warn(f\"Train set has ({len(self.train_dataset)} samples), consider using get_train_dataloader() instead\")\n    train_dataloader = self.get_train_dataloader()\n    assert isinstance(train_dataloader.sampler, BatchSampler) and self.train_dataloader_sampler is not None\n    # Read dataloader in sequential order\n    train_dataloader.sampler.sampler = SequentialSampler(self.train_dataset)\n    train_dataloader.sampler.drop_last = False\n    feature_names = self.dataset_config.get_feature_names(flatten_ppi=flatten_ppi)\n    df = create_df_from_dataloader(dataloader=train_dataloader, feature_names=feature_names, flatten_ppi=flatten_ppi, silent=self.silent)\n    # Restore the original dataloader sampler and drop_last\n    train_dataloader.sampler.sampler = self.train_dataloader_sampler\n    train_dataloader.sampler.drop_last = self.train_dataloader_drop_last\n    return df\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.get_val_df","title":"get_val_df","text":"<pre><code>get_val_df(flatten_ppi: bool = False) -&gt; pd.DataFrame\n</code></pre> <p>Creates validation Pandas <code>DataFrame</code>. The dataframe is in sequential (datetime) order.</p> <p>Memory usage</p> <p>The whole validation set is loaded into memory. If the dataset size is larger than <code>'S'</code>, consider using <code>get_val_dataloader</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>flatten_ppi</code> <code>bool</code> <p>Whether to flatten the PPI sequence into individual columns (named <code>IPT_X</code>, <code>DIR_X</code>, <code>SIZE_X</code>, <code>PUSH_X</code>, X being the index of the packet) or keep one <code>PPI</code> column with 2D data.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Validation data as a dataframe.</p> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_df(self, flatten_ppi: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates validation Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The dataframe is in sequential (datetime) order.\n\n    !!! warning \"Memory usage\"\n\n        The whole validation set is loaded into memory. If the dataset size is larger than `'S'`, consider using `get_val_dataloader` instead.\n\n    Parameters:\n        flatten_ppi: Whether to flatten the PPI sequence into individual columns (named `IPT_X`, `DIR_X`, `SIZE_X`, `PUSH_X`, *X* being the index of the packet) or keep one `PPI` column with 2D data.\n\n    Returns:\n        Validation data as a dataframe.\n    \"\"\"\n    self._check_before_dataframe()\n    assert self.dataset_config is not None and self.val_dataset is not None\n    if len(self.val_dataset) &gt; DATAFRAME_SAMPLES_WARNING_THRESHOLD:\n        warnings.warn(f\"Validation set has ({len(self.val_dataset)} samples), consider using get_val_dataloader() instead\")\n    feature_names = self.dataset_config.get_feature_names(flatten_ppi=flatten_ppi)\n    return create_df_from_dataloader(dataloader=self.get_val_dataloader(), feature_names=feature_names, flatten_ppi=flatten_ppi, silent=self.silent)\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.get_test_df","title":"get_test_df","text":"<pre><code>get_test_df(flatten_ppi: bool = False) -&gt; pd.DataFrame\n</code></pre> <p>Creates test Pandas <code>DataFrame</code>. The dataframe is in sequential (datetime) order.</p> <p>When the dataset is used in the open-world setting, and unknown classes are defined, the returned test dataframe is composed of <code>test_known_size</code> samples of known classes followed by <code>test_unknown_size</code> samples of unknown classes.</p> <p>Memory usage</p> <p>The whole test set is loaded into memory. If the dataset size is larger than <code>'S'</code>, consider using <code>get_test_dataloader</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>flatten_ppi</code> <code>bool</code> <p>Whether to flatten the PPI sequence into individual columns (named <code>IPT_X</code>, <code>DIR_X</code>, <code>SIZE_X</code>, <code>PUSH_X</code>, X being the index of the packet) or keep one <code>PPI</code> column with 2D data.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Test data as a dataframe.</p> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_df(self, flatten_ppi: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates test Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The dataframe is in sequential (datetime) order.\n\n\n    When the dataset is used in the open-world setting, and unknown classes are defined,\n    the returned test dataframe is composed of `test_known_size` samples of known classes followed by `test_unknown_size` samples of unknown classes.\n\n\n    !!! warning \"Memory usage\"\n\n        The whole test set is loaded into memory. If the dataset size is larger than `'S'`, consider using `get_test_dataloader` instead.\n\n    Parameters:\n        flatten_ppi: Whether to flatten the PPI sequence into individual columns (named `IPT_X`, `DIR_X`, `SIZE_X`, `PUSH_X`, *X* being the index of the packet) or keep one `PPI` column with 2D data.\n\n    Returns:\n        Test data as a dataframe.\n    \"\"\"\n    self._check_before_dataframe()\n    assert self.dataset_config is not None and self.test_dataset is not None\n    if len(self.test_dataset) &gt; DATAFRAME_SAMPLES_WARNING_THRESHOLD:\n        warnings.warn(f\"Test set has ({len(self.test_dataset)} samples), consider using get_test_dataloader() instead\")\n    feature_names = self.dataset_config.get_feature_names(flatten_ppi=flatten_ppi)\n    return create_df_from_dataloader(dataloader=self.get_test_dataloader(), feature_names=feature_names, flatten_ppi=flatten_ppi, silent=self.silent)\n</code></pre>"},{"location":"reference_cesnet_dataset/#datasets.cesnet_dataset.CesnetDataset.compute_dataset_statistics","title":"compute_dataset_statistics","text":"<pre><code>compute_dataset_statistics(\n    num_samples: int | Literal[\"all\"] = 10000000,\n    num_workers: int = 4,\n    batch_size: int = 4096,\n    disabled_apps: Optional[list[str]] = None,\n) -&gt; None\n</code></pre> <p>Computes dataset statistics and saves them to the <code>statistics_path</code> folder.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int | Literal['all']</code> <p>Number of samples to use for computing the statistics.</p> <code>10000000</code> <code>num_workers</code> <code>int</code> <p>Number of workers for loading data.</p> <code>4</code> <code>batch_size</code> <code>int</code> <p>Number of samples per batch for loading data.</p> <code>4096</code> <code>disabled_apps</code> <code>Optional[list[str]]</code> <p>List of applications to exclude from the statistics.</p> <code>None</code> Source code in <code>cesnet_datazoo\\datasets\\cesnet_dataset.py</code> <pre><code>def compute_dataset_statistics(self, num_samples: int | Literal[\"all\"] = 10_000_000, num_workers: int = 4, batch_size: int = 4096, disabled_apps: Optional[list[str]] = None)-&gt; None:\n    \"\"\"\n    Computes dataset statistics and saves them to the `statistics_path` folder.\n\n    Parameters:\n        num_samples: Number of samples to use for computing the statistics.\n        num_workers: Number of workers for loading data.\n        batch_size: Number of samples per batch for loading data.\n        disabled_apps: List of applications to exclude from the statistics.\n    \"\"\"\n    if self.name.startswith(\"CESNET-TLS22\"):\n        raise NotImplementedError(\"Dataset statistics are not supported for CESNET_TLS22\")\n    flowstats_features = self.metadata.flowstats_features + self.metadata.packet_histogram_features + self.metadata.tcp_features\n    if not os.path.exists(self.statistics_path):\n        os.mkdir(self.statistics_path)\n    compute_dataset_statistics(database_path=self.database_path,\n                               output_dir=self.statistics_path,\n                               flowstats_features=flowstats_features,\n                               protocol=self.metadata.protocol,\n                               disabled_apps=disabled_apps if disabled_apps is not None else [],\n                               num_samples=num_samples,\n                               num_workers=num_workers,\n                               batch_size=batch_size,\n                               silent=self.silent)\n</code></pre>"},{"location":"reference_dataset_config/","title":"Config class","text":""},{"location":"reference_dataset_config/#config.DatasetConfig","title":"config.DatasetConfig","text":"<p>The main class for the configuration of:</p> <ul> <li>Train, validation, test sets (dates, sizes, validation approach).</li> <li>Application selection \u2014 either the standard closed-world setting (only known classes) or the open-world setting (known and unknown classes).</li> <li>Feature scaling.</li> <li>Dataloader options like batch sizes, order of loading, or number of workers.</li> </ul> <p>When initializing this class, pass a <code>CesnetDataset</code> instance to be configured and the desired configuration. Available options are here.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>InitVar[CesnetDataset]</code> <p>The dataset instance to be configured</p> <code>data_root</code> <code>str</code> <p>Extracted from the dataset instance</p> <code>database_filename</code> <code>str</code> <p>Extracted from the dataset instance</p> <code>database_path</code> <code>str</code> <p>Extracted from the dataset instance</p> <code>servicemap_path</code> <code>str</code> <p>Extracted from the dataset instance</p> <code>flowstats_features</code> <code>list[str]</code> <p>Extracted from the <code>dataset.metadata.flowstats_features</code></p>"},{"location":"reference_dataset_config/#config.DatasetConfig--configuration-options","title":"Configuration options","text":"<p>Attributes:</p> Name Type Description <code>train_period</code> <code>str</code> <p>Name of the train period. See instructions.</p> <code>train_dates</code> <code>list[str]</code> <p>Dates used for creating a train set.</p> <code>train_dates_weigths</code> <code>Optional[list[int]]</code> <p>To use a non-uniform distribution of samples across train dates.</p> <code>val_approach</code> <code>ValidationApproach</code> <p>How a validation set should be created. Either split train data into train and validation, have a separate validation period, or no validation at all. <code>Default: SPLIT_FROM_TRAIN</code></p> <code>train_val_split_fraction</code> <code>float</code> <p>The fraction of validation samples when splitting from the train set. <code>Default: 0.2</code></p> <code>val_period</code> <code>str</code> <p>Name of the validation period. See instructions.</p> <code>val_dates</code> <code>list[str]</code> <p>Dates used for creating a validation set.</p> <code>test_period</code> <code>str</code> <p>Name of the test period. See instructions.</p> <code>test_dates</code> <code>list[str]</code> <p>Dates used for creating a test set.</p> <code>apps_selection</code> <code>AppSelection</code> <p>How to select application classes. <code>Default: ALL_KNOWN</code></p> <code>apps_selection_topx</code> <code>int</code> <p>Take top X as known.</p> <code>apps_selection_explicit_unknown</code> <code>list[str]</code> <p>Provide a list of unknown applications.</p> <code>apps_selection_fixed_longterm</code> <code>Optional[tuple[dict[int, str], dict[int, str]]]</code> <p>Provide enums of known and unknown applications. This is suitable for long-term measurements.</p> <code>disabled_apps</code> <code>list[str]</code> <p>List of applications to be disabled and not used at all.</p> <code>min_train_samples_check</code> <code>MinTrainSamplesCheck</code> <p>How to handle applications with not enough training samples. <code>Default: DISABLE_APPS</code></p> <code>min_train_samples_per_app</code> <code>int</code> <p>Defines the threshold for not enough. <code>Default: 100</code></p> <code>random_state</code> <code>int</code> <p>Fix all random processes performed during dataset initialization. <code>Default: 420</code></p> <code>fold_id</code> <code>int</code> <p>To perform N-fold cross-validation, set this to <code>1..N</code>. Each fold will use the same configuration but a different random seed. <code>Default: 0</code></p> <code>train_workers</code> <code>int</code> <p>Number of workers for loading train data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>test_workers</code> <code>int</code> <p>Number of workers for loading test data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 1</code></p> <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 1</code></p> <code>batch_size</code> <code>int</code> <p>Number of samples per batch. <code>Default: 192</code></p> <code>test_batch_size</code> <code>int</code> <p>Number of samples per batch for loading validation and test data. <code>Default: 2048</code></p> <code>preload_test</code> <code>bool</code> <p>Whether to dump the test set with <code>numpy.savez_compressed</code> and preload it in future runs. Useful when running a lot of experiments with the same dataset configuration. <code>Default: False</code></p> <code>train_size</code> <code>int | Literal['all']</code> <p>Size of the train set. See instructions. <code>Default: all</code></p> <code>val_known_size</code> <code>int | Literal['all']</code> <p>Size of the validation set. See instructions. <code>Default: all</code></p> <code>test_known_size</code> <code>int | Literal['all']</code> <p>Size of the test set. See instructions. <code>Default: all</code></p> <code>val_unknown_size</code> <code>int | Literal['all']</code> <p>Size of the unknown classes validation set. Use for evaluation in the open-world setting. <code>Default: 0</code></p> <code>test_unknown_size</code> <code>int | Literal['all']</code> <p>Size of the unknown classes test set. Use for evaluation in the open-world setting. <code>Default: 0</code></p> <code>train_dataloader_order</code> <code>DataLoaderOrder</code> <p>Whether to load train data in sequential or random order. <code>Default: RANDOM</code></p> <code>train_dataloader_seed</code> <code>Optional[int]</code> <p>Seed for loading train data in random order. <code>Default: None</code></p> <code>return_ips</code> <code>bool</code> <p>Use for IP-based classification. Dataloaders will return data in this tuple format <code>((SRC_IP, DST_IP, SRC_PORT, DST_PORT), LABELS)</code>. Dataframes are not available when this option is used. <code>Default: False</code></p> <code>return_torch</code> <code>bool</code> <p>Use for returning <code>torch.Tensor</code> from dataloaders. Dataframes are not available when this option is used. <code>Default: False</code></p> <code>use_packet_histograms</code> <code>bool</code> <p>Whether to use packet histogram features, if available in the dataset. <code>Default: True</code></p> <code>use_tcp_features</code> <code>bool</code> <p>Whether to use TCP features, if available in the dataset. <code>Default: True</code></p> <code>use_push_flags</code> <code>bool</code> <p>Whether to use push flags in packet sequences, if available in the dataset. <code>Default: False</code></p> <code>zero_ppi_start</code> <code>int</code> <p>Zeroing out the first N packets of each packet sequence. <code>Default: 0</code></p> <code>fit_scalers_samples</code> <code>int | float</code> <p>Fraction of train samples used for fitting feature scalers, if float. The absolute number of samples otherwise. <code>Default: 0.25</code></p> <code>flowstats_scaler</code> <code>ScalerEnum</code> <p>Which scaler to use for flow statistics. Options are <code>ROBUST</code> | <code>STANDARD</code> | <code>MINMAX</code> | <code>NO_SCALER</code>. <code>Default: ROBUST</code></p> <code>flowstats_clip</code> <code>float</code> <p>Quantile clip before the scaling of flow statistics. Should limit the influence of outliers. Set to <code>1</code> to disable. <code>Default: 0.99</code></p> <code>psizes_scaler</code> <code>ScalerEnum</code> <p>Which scaler to use for packet sizes. Options are <code>ROBUST</code> | <code>STANDARD</code> | <code>MINMAX</code> | <code>NO_SCALER</code>. <code>Default: STANDARD</code></p> <code>psizes_max</code> <code>int</code> <p>Max clip packet sizes before scaling. <code>Default: 1460</code></p> <code>ipt_scaler</code> <code>ScalerEnum</code> <p>Which scaler to use for inter-packet times. Options are <code>ROBUST</code> | <code>STANDARD</code> | <code>MINMAX</code> | <code>NO_SCALER</code>. <code>Default: STANDARD</code></p> <code>ipt_min</code> <code>int</code> <p>Min clip inter-packet times before scaling. <code>Default: 0</code></p> <code>ipt_max</code> <code>int</code> <p>Max clip inter-packet times before scaling. <code>Default: 15000</code></p>"},{"location":"reference_dataset_config/#config.DatasetConfig--how-to-configure-train-validation-and-test-sets","title":"How to configure train, validation, and test sets","text":"<p>There are three options for how to define train/validation/test time periods and dates.</p> <ol> <li>Specify a predefined time period (<code>train_period</code>, <code>val_period</code>, or <code>test_period</code>) available in <code>dataset.time_periods</code> and leave the list of dates (<code>train_dates</code>, <code>val_dates</code>, or <code>test_dates</code>) empty.</li> <li>Name a custom time period and provide a list of dates. The dates are checked against <code>dataset.available_dates</code>.</li> <li>Leave everything empty and use the dataset's defaults <code>dataset.default_train_period</code> and <code>dataset.default_test_period</code>.</li> </ol> <p>There are two options for configuring sizes of train/validation/test sets.</p> <ol> <li>Select an appropriate dataset size (default is <code>S</code>) when creating the <code>CesnetDataset</code> instance and leave <code>train_size</code>, <code>val_known_size</code>, and <code>test_known_size</code> with their default <code>all</code> value. This will create train/validation/test sets with all samples available in the selected dataset size (of course, depending on the selected dates and validation approach).</li> <li>Provide exact sizes in <code>train_size</code>, <code>val_known_size</code>, and <code>test_known_size</code>. This will create train/validation/test sets of the given sizes by doing a random subset. This is especially useful when using the <code>ORIG</code> dataset size and want to run smaller experiments.</li> </ol> <p>Tip</p> <p>The default approach for creating a validation set is to randomly split the train data into train and validation. The second approach is to define separate validation dates. See ValidationApproach.</p> Source code in <code>cesnet_datazoo\\config.py</code> <pre><code>@dataclass(config=C)\nclass DatasetConfig():\n    \"\"\"\n    The main class for the configuration of:\n\n    - Train, validation, test sets (dates, sizes, validation approach).\n    - Application selection \u2014 either the standard closed-world setting (only *known* classes) or the open-world setting (*known* and *unknown* classes).\n    - Feature scaling.\n    - Dataloader options like batch sizes, order of loading, or number of workers.\n\n    When initializing this class, pass a [`CesnetDataset`][datasets.cesnet_dataset.CesnetDataset] instance to be configured and the desired configuration. Available options are [here][config.DatasetConfig--configuration-options].\n\n    Attributes:\n        dataset: The dataset instance to be configured\n        data_root: Extracted from the dataset instance\n        database_filename: Extracted from the dataset instance\n        database_path: Extracted from the dataset instance\n        servicemap_path: Extracted from the dataset instance\n        flowstats_features: Extracted from the `dataset.metadata.flowstats_features`\n\n    # Configuration options\n\n    Attributes:\n        train_period: Name of the train period. See [instructions][config.DatasetConfig--how-to-configure-train-validation-and-test-sets].\n        train_dates: Dates used for creating a train set.\n        train_dates_weigths: To use a non-uniform distribution of samples across train dates.\n        val_approach: How a validation set should be created. Either split train data into train and validation, have a separate validation period, or no validation at all. `Default: SPLIT_FROM_TRAIN`\n        train_val_split_fraction: The fraction of validation samples when splitting from the train set. `Default: 0.2`\n        val_period: Name of the validation period. See [instructions][config.DatasetConfig--how-to-configure-train-validation-and-test-sets].\n        val_dates: Dates used for creating a validation set.\n        test_period: Name of the test period. See [instructions][config.DatasetConfig--how-to-configure-train-validation-and-test-sets].\n        test_dates: Dates used for creating a test set.\n\n        apps_selection: How to select application classes. `Default: ALL_KNOWN`\n        apps_selection_topx: Take top X as known.\n        apps_selection_explicit_unknown: Provide a list of unknown applications.\n        apps_selection_fixed_longterm: Provide enums of known and unknown applications. This is suitable for long-term measurements.\n        disabled_apps: List of applications to be disabled and not used at all.\n        min_train_samples_check: How to handle applications with *not enough* training samples. `Default: DISABLE_APPS`\n        min_train_samples_per_app: Defines the threshold for *not enough*. `Default: 100`\n\n        random_state: Fix all random processes performed during dataset initialization. `Default: 420`\n        fold_id: To perform N-fold cross-validation, set this to `1..N`. Each fold will use the same configuration but a different random seed. `Default: 0`\n        train_workers: Number of workers for loading train data. `0` means that the data will be loaded in the main process. `Default: 4`\n        test_workers: Number of workers for loading test data. `0` means that the data will be loaded in the main process. `Default: 1`\n        val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process. `Default: 1`\n        batch_size: Number of samples per batch. `Default: 192`\n        test_batch_size: Number of samples per batch for loading validation and test data. `Default: 2048`\n        preload_test: Whether to dump the test set with `numpy.savez_compressed` and preload it in future runs. Useful when running a lot of experiments with the same dataset configuration. `Default: False`\n        train_size: Size of the train set. See [instructions][config.DatasetConfig--how-to-configure-train-validation-and-test-sets]. `Default: all`\n        val_known_size: Size of the validation set. See [instructions][config.DatasetConfig--how-to-configure-train-validation-and-test-sets]. `Default: all`\n        test_known_size: Size of the test set. See [instructions][config.DatasetConfig--how-to-configure-train-validation-and-test-sets]. `Default: all`\n        val_unknown_size: Size of the unknown classes validation set. Use for evaluation in the open-world setting. `Default: 0`\n        test_unknown_size: Size of the unknown classes test set. Use for evaluation in the open-world setting. `Default: 0`\n        train_dataloader_order: Whether to load train data in sequential or random order. `Default: RANDOM`\n        train_dataloader_seed: Seed for loading train data in random order. `Default: None`\n\n        return_ips: Use for IP-based classification. Dataloaders will return data in this tuple format `((SRC_IP, DST_IP, SRC_PORT, DST_PORT), LABELS)`. Dataframes are not available when this option is used. `Default: False`\n        return_torch: Use for returning `torch.Tensor` from dataloaders. Dataframes are not available when this option is used. `Default: False`\n        use_packet_histograms: Whether to use packet histogram features, if available in the dataset. `Default: True`\n        use_tcp_features: Whether to use TCP features, if available in the dataset. `Default: True`\n        use_push_flags: Whether to use push flags in packet sequences, if available in the dataset. `Default: False`\n        zero_ppi_start: Zeroing out the first N packets of each packet sequence. `Default: 0`\n        fit_scalers_samples: Fraction of train samples used for fitting feature scalers, if float. The absolute number of samples otherwise. `Default: 0.25`\n        flowstats_scaler: Which scaler to use for flow statistics. Options are [`ROBUST`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) | [`STANDARD`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) | [`MINMAX`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) | `NO_SCALER`. `Default: ROBUST`\n        flowstats_clip: Quantile clip before the scaling of flow statistics. Should limit the influence of outliers. Set to `1` to disable. `Default: 0.99`\n        psizes_scaler: Which scaler to use for packet sizes. Options are [`ROBUST`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) | [`STANDARD`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) | [`MINMAX`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) | `NO_SCALER`. `Default: STANDARD`\n        psizes_max: Max clip packet sizes before scaling. `Default: 1460`\n        ipt_scaler: Which scaler to use for inter-packet times. Options are [`ROBUST`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) | [`STANDARD`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) | [`MINMAX`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) | `NO_SCALER`. `Default: STANDARD`\n        ipt_min: Min clip inter-packet times before scaling. `Default: 0`\n        ipt_max: Max clip inter-packet times before scaling. `Default: 15000`\n\n    # How to configure train, validation, and test sets\n    There are three options for how to define train/validation/test __time periods and dates__.\n\n    1. Specify a predefined time period (`train_period`, `val_period`, or `test_period`) available in `dataset.time_periods` and leave the list of dates (`train_dates`, `val_dates`, or `test_dates`) empty.\n    2. Name a custom time period and provide a list of dates. The dates are checked against `dataset.available_dates`.\n    3. Leave everything empty and use the dataset's defaults `dataset.default_train_period` and `dataset.default_test_period`.\n\n    There are two options for configuring __sizes__ of train/validation/test sets.\n\n    1. Select an appropriate dataset size (default is `S`) when creating the [`CesnetDataset`][datasets.cesnet_dataset.CesnetDataset] instance and leave `train_size`, `val_known_size`, and `test_known_size` with their default `all` value.\n    This will create train/validation/test sets with all samples available in the selected dataset size (of course, depending on the selected dates and validation approach).\n    2. Provide exact sizes in `train_size`, `val_known_size`, and `test_known_size`. This will create train/validation/test sets of the given sizes by doing a random subset.\n    This is especially useful when using the `ORIG` dataset size and want to run smaller experiments.\n\n    !!! tip Validation set\n        The default approach for creating a validation set is to randomly split the train data into train and validation. The second approach is to define separate validation dates. See [ValidationApproach][config.ValidationApproach].\n\n    \"\"\"\n    dataset: InitVar[CesnetDataset]\n    data_root: str = field(init=False)\n    database_filename: str =  field(init=False)\n    database_path: str =  field(init=False)\n    servicemap_path: str = field(init=False)\n    flowstats_features: list[str] = field(init=False)\n\n    train_period: str = \"\"\n    train_dates: list[str] = field(default_factory=list)\n    train_dates_weigths: Optional[list[int]] = None\n    val_approach: ValidationApproach = ValidationApproach.SPLIT_FROM_TRAIN\n    train_val_split_fraction: float = 0.2\n    val_period: str = \"\"\n    val_dates: list[str] = field(default_factory=list)\n    test_period: str = \"\"\n    test_dates: list[str] = field(default_factory=list)\n\n    apps_selection: AppSelection = AppSelection.ALL_KNOWN\n    apps_selection_topx: int = 0\n    apps_selection_explicit_unknown: list[str] = field(default_factory=list)\n    apps_selection_fixed_longterm: Optional[tuple[dict[int, str], dict[int, str]]] = None\n    disabled_apps: list[str] = field(default_factory=list)\n    min_train_samples_check: MinTrainSamplesCheck = MinTrainSamplesCheck.DISABLE_APPS\n    min_train_samples_per_app: int = 100\n\n    random_state: int = 420\n    fold_id: int = 0\n    train_workers: int = 4\n    test_workers: int = 1\n    val_workers: int = 1\n    batch_size: int = 192\n    test_batch_size: int = 2048\n    preload_test: bool = False\n    train_size: int | Literal[\"all\"] = \"all\"\n    val_known_size: int | Literal[\"all\"] = \"all\"\n    test_known_size: int | Literal[\"all\"] = \"all\"\n    val_unknown_size: int | Literal[\"all\"] = 0\n    test_unknown_size: int | Literal[\"all\"] = 0\n    train_dataloader_order: DataLoaderOrder = DataLoaderOrder.RANDOM\n    train_dataloader_seed: Optional[int] = None\n\n    return_ips: bool = False\n    return_torch: bool = False\n    use_packet_histograms: bool = True\n    use_tcp_features: bool = True\n    use_push_flags: bool = False\n    zero_ppi_start: int = 0\n    fit_scalers_samples: int | float = 0.25\n    flowstats_scaler: ScalerEnum = ScalerEnum.ROBUST\n    flowstats_clip: float = 0.99\n    psizes_scaler: ScalerEnum = ScalerEnum.STANDARD\n    psizes_max: int = 1460\n    ipt_scaler: ScalerEnum = ScalerEnum.STANDARD\n    ipt_min: int = 0\n    ipt_max: int = 15000\n\n    def __post_init__(self, dataset: CesnetDataset):\n        \"\"\"\n        Ensures valid configuration. Catches all incompatible options and raise exceptions as soon as possible.\n        \"\"\"\n        self.data_root = dataset.data_root\n        self.servicemap_path = dataset.servicemap_path\n        self.database_filename = dataset.database_filename\n        self.database_path = dataset.database_path\n        self.flowstats_features = dataset.metadata.flowstats_features\n\n        # Configure train dates and period\n        if len(self.train_dates) &gt; 0 and self.train_period == \"\":\n            raise ValueError(\"train_period has to be specified when train_dates are set\")\n        if len(self.train_dates) == 0 and self.train_period != \"\":\n            if self.train_period not in dataset.time_periods:\n                raise ValueError(f\"Unknown train_period {self.train_period}. Use time period available in dataset.time_periods\")\n            self.train_dates = dataset.time_periods[self.train_period]\n        if len(self.train_dates) == 0 and self.test_period == \"\":\n            self.train_period = dataset.default_train_period\n            self.train_dates = dataset.time_periods[dataset.default_train_period]\n        # Configure test dates and period\n        if len(self.test_dates) &gt; 0 and self.test_period == \"\":\n            raise ValueError(\"test_period has to be specified when test_dates are set\")\n        if len(self.test_dates) == 0 and self.test_period != \"\":\n            if self.test_period not in dataset.time_periods:\n                raise ValueError(f\"Unknown test_period {self.test_period}. Use time period available in dataset.time_periods\")\n            self.test_dates = dataset.time_periods[self.test_period]\n        if len(self.test_dates) == 0 and self.test_period == \"\":\n            self.test_period = dataset.default_test_period\n            self.test_dates = dataset.time_periods[dataset.default_test_period]\n        # Configure val dates and period\n        if (self.val_approach == ValidationApproach.NO_VALIDATION or self.val_approach == ValidationApproach.SPLIT_FROM_TRAIN) and (len(self.val_dates) &gt; 0 or self.val_period != \"\"):\n            raise ValueError(\"val_dates and val_period cannot be specified when val_approach is no-validation or split-from-train\")\n        if self.val_approach == ValidationApproach.VALIDATION_DATES:\n            if len(self.val_dates) &gt; 0 and self.val_period == \"\":\n                raise ValueError(\"val_period has to be specified when val_dates are set\")\n            if len(self.val_dates) == 0 and self.val_period != \"\":\n                if self.val_period not in dataset.time_periods:\n                    raise ValueError(f\"Unknown val_period {self.val_period}. Use time period available in dataset.time_periods\")\n                self.val_dates = dataset.time_periods[self.val_period]\n            if len(self.val_dates) == 0 and self.val_period == \"\":\n                raise ValueError(\"val_period and val_dates (or val_period from dataset.time_periods) have to be specified when val_approach is validation-dates\")\n        # Check if train, val, and test dates are available in the dataset\n        if dataset.available_dates:\n            unknown_train_dates = [t for t in self.train_dates if t not in dataset.available_dates]\n            unknown_val_dates = [t for t in self.val_dates if t not in dataset.available_dates]\n            unknown_test_dates = [t for t in self.test_dates if t not in dataset.available_dates]\n            if len(unknown_train_dates) &gt; 0:\n                raise ValueError(f\"Unknown train dates {unknown_train_dates}. Use dates available in dataset.available_dates\" \\\n                                + f\". These dates are missing from the dataset collection period {dataset.metadata.missing_dates_in_collection_period}\" if dataset.metadata.missing_dates_in_collection_period else \"\")\n            if len(unknown_val_dates) &gt; 0:\n                raise ValueError(f\"Unknown validation dates {unknown_val_dates}. Use dates available in dataset.available_dates\" \\\n                                + f\". These dates are missing from the dataset collection period {dataset.metadata.missing_dates_in_collection_period}\" if dataset.metadata.missing_dates_in_collection_period else \"\")\n            if len(unknown_test_dates) &gt; 0:\n                raise ValueError(f\"Unknown test dates {unknown_test_dates}. Use dates available in dataset.available_dates\" \\\n                                + f\". These dates are missing from the dataset collection period {dataset.metadata.missing_dates_in_collection_period}\" if dataset.metadata.missing_dates_in_collection_period else \"\")\n        # Configure features\n        if self.use_packet_histograms:\n            if len(dataset.metadata.packet_histogram_features) &gt; 0:\n                self.flowstats_features = self.flowstats_features + dataset.metadata.packet_histogram_features\n            else:\n                self.use_packet_histograms = False\n        if dataset.metadata.protocol == Protocol.TLS and self.use_tcp_features:\n            self.flowstats_features = self.flowstats_features + SELECTED_TCP_FLAGS\n            if self.use_push_flags and \"PUSH_FLAG\" not in dataset.metadata.features_in_packet_sequences:\n                raise ValueError(\"This TLS dataset does not support use_push_flags\")\n        if dataset.metadata.protocol == Protocol.QUIC:\n            self.use_tcp_features = False\n            if self.use_push_flags:\n                raise ValueError(\"QUIC datasets do not support use_push_flags\")\n        # When train_dates_weigths are used, train_size and val_known_size have to be specified\n        if self.train_dates_weigths is not None:\n            if len(self.train_dates_weigths) != len(self.train_dates):\n                raise ValueError(\"train_dates_weigths has to have the same length as train_dates\")\n            if self.train_size == \"all\":\n                raise ValueError(\"train_size cannot be 'all' when train_dates_weigths are speficied\")\n            if self.val_approach == ValidationApproach.SPLIT_FROM_TRAIN and self.val_known_size == \"all\":\n                raise ValueError(\"val_known_size cannot be 'all' when train_dates_weigths are speficied and validation_approach is split-from-train\")\n        # App selection\n        if self.apps_selection == AppSelection.ALL_KNOWN:\n            self.val_unknown_size = 0\n            self.test_unknown_size = 0\n            if self.apps_selection_topx != 0 or len(self.apps_selection_explicit_unknown) &gt; 0 or self.apps_selection_fixed_longterm is not None:\n                raise ValueError(\"apps_selection_topx, apps_selection_explicit_unknown, and apps_selection_fixed_longterm cannot be specified when apps_selection is all-known\")\n        if self.apps_selection == AppSelection.TOPX_KNOWN and self.apps_selection_topx == 0:\n            raise ValueError(\"apps_selection_topx has to be greater than 0 when apps_selection is top-x-known\")\n        if self.apps_selection == AppSelection.EXPLICIT_UNKNOWN and len(self.apps_selection_explicit_unknown) == 0:\n            raise ValueError(\"apps_selection_explicit_unknown has to be specified when apps_selection is explicit-unknown\")\n        if self.apps_selection == AppSelection.LONGTERM_FIXED:\n            if self.apps_selection_fixed_longterm is None:\n                raise ValueError(\"apps_selection_fixed_longterm, a tuple of (known_apps_database_enum, unknown_apps_database_enum), has to be specified when apps_selection is longterm-fixed\")\n            if len(self.disabled_apps) &gt; 0:\n                raise ValueError(\"disabled_apps cannot be specified when apps_selection is longterm-fixed\")\n            if self.min_train_samples_per_app != 0:\n                raise ValueError(\"min_train_samples_per_app has to be 0 when apps_selection is longterm-fixed\")\n        if sum((self.apps_selection_topx != 0, len(self.apps_selection_explicit_unknown) &gt; 0, self.apps_selection_fixed_longterm is not None)) &gt; 1:\n            raise ValueError(\"apps_selection_topx, apps_selection_explicit_unknown, and apps_selection_fixed_longterm should not be specified at the same time\")\n        # More asserts\n        if self.zero_ppi_start &gt; PPI_MAX_LEN:\n            raise ValueError(f\"zero_ppi_start has to be &lt;= {PPI_MAX_LEN}\")\n        if isinstance(self.fit_scalers_samples, float) and (self.fit_scalers_samples &lt;= 0 or self.fit_scalers_samples &gt; 1):\n            raise ValueError(\"fit_scalers_samples has to be either float between 0 and 1 (giving the fraction of training samples used for fitting scalers) or an integer\")\n\n    def get_flowstats_features_len(self) -&gt; int:\n        \"\"\"Gets the number of flow statistics features.\"\"\"\n        n = 0\n        for f in self.flowstats_features:\n            if f.startswith(\"PHIST_\"):\n                n += PHIST_BIN_COUNT\n            else:\n                n += 1\n        return n\n\n    def get_flowstats_feature_names_expanded(self) -&gt; list[str]:\n        \"\"\"Gets names of flow statistics features. Packet histograms are expanded into individual features.\"\"\"\n        name_mapping = {\n            \"PHIST_SRC_SIZES\": [f\"PSIZE_BIN{i}\" for i in range(1, PHIST_BIN_COUNT + 1)],\n            \"PHIST_DST_SIZES\": [f\"PSIZE_BIN{i}_REV\" for i in range(1, PHIST_BIN_COUNT + 1)],\n            \"PHIST_SRC_IPT\": [f\"IPT_BIN{i}\" for i in range(1, PHIST_BIN_COUNT + 1)],\n            \"PHIST_DST_IPT\": [f\"IPT_BIN{i}_REV\" for i in range(1, PHIST_BIN_COUNT + 1)],\n            \"FLOW_ENDREASON_IDLE\": \"FEND_IDLE\",\n            \"FLOW_ENDREASON_ACTIVE\": \"FEND_ACTIVE\",\n            \"FLOW_ENDREASON_END\": \"FEND_END\",\n            \"FLOW_ENDREASON_OTHER\": \"FEND_OTHER\",\n        }\n        feature_names = []\n        for f in self.flowstats_features:\n            if f not in name_mapping:\n                f = f if not f.startswith(\"FLAG\") else \"F\" + f.lstrip(\"FLAG\")\n                f = f.upper()\n                feature_names.append(f)\n            elif isinstance(name_mapping[f], list):\n                feature_names.extend(name_mapping[f])\n            else:\n                feature_names.append(name_mapping[f])\n        assert len(feature_names) == self.get_flowstats_features_len()\n        return feature_names\n\n    def get_ppi_feature_names(self) -&gt; list[str]:\n        \"\"\"Gets names of flattened per-packet information (PPI) features.\"\"\"\n        ppi_feature_names = [f\"IPT_{i}\" for i in range(1, PPI_MAX_LEN + 1)] + \\\n                               [f\"DIR_{i}\" for i in range(1, PPI_MAX_LEN + 1)] + \\\n                               [f\"SIZE_{i}\" for i in range(1, PPI_MAX_LEN + 1)]\n        if self.use_push_flags:\n            ppi_feature_names += [f\"PUSH_{i}\" for i in range(1, PPI_MAX_LEN + 1)]\n        return ppi_feature_names\n\n    def get_ppi_channels(self) -&gt; int:\n        \"\"\"Gets the number of per-packet information (PPI) channels.\"\"\"\n        if self.use_push_flags:\n            return TCP_PPI_CHANNELS\n        else:\n            return UDP_PPI_CHANNELS\n\n    def get_feature_names(self, flatten_ppi: bool = False) -&gt; list[str]:\n        \"\"\"\n        Gets feature names.\n\n        Parameters:\n            flatten_ppi: Whether to flatten PPI into individual feature names or keep one `PPI` column.\n        \"\"\"\n        feature_names = self.get_ppi_feature_names() if flatten_ppi else [\"PPI\"]\n        feature_names += self.get_flowstats_feature_names_expanded()\n        return feature_names\n\n    def _get_train_tables_paths(self) -&gt; list[str]:\n        return list(map(lambda t: f\"/flows/D{t}\", self.train_dates))\n\n    def _get_val_tables_paths(self) -&gt; list[str]:\n        if self.val_approach == ValidationApproach.SPLIT_FROM_TRAIN:\n            return list(map(lambda t: f\"/flows/D{t}\", self.train_dates))\n        return list(map(lambda t: f\"/flows/D{t}\", self.val_dates))\n\n    def _get_test_tables_paths(self) -&gt; list[str]:\n        return list(map(lambda t: f\"/flows/D{t}\", self.test_dates))\n\n    def _get_train_data_hash(self) -&gt; str:\n        train_data_params = self._get_train_data_params()\n        params_hash = hashlib.sha256(json.dumps(dataclasses.asdict(train_data_params), sort_keys=True, default=str).encode()).hexdigest()\n        params_hash = params_hash[:10]\n        return params_hash\n\n    def _get_train_data_path(self) -&gt; str:\n        params_hash = self._get_train_data_hash()\n        return os.path.join(self.data_root, \"train-data\", f\"{params_hash}_{self.random_state}\", f\"fold_{self.fold_id}\")\n\n    def _get_train_data_params(self) -&gt; TrainDataParams:\n        return TrainDataParams(\n            database_filename=self.database_filename,\n            train_period=self.train_period,\n            train_tables_paths=self._get_train_tables_paths(),\n            apps_selection=self.apps_selection,\n            apps_selection_topx=self.apps_selection_topx,\n            apps_selection_explicit_unknown=self.apps_selection_explicit_unknown,\n            apps_selection_fixed_longterm=self.apps_selection_fixed_longterm,\n            disabled_apps=self.disabled_apps,\n            min_train_samples_per_app=self.min_train_samples_per_app,\n            min_train_samples_check=self.min_train_samples_check,)\n\n    def _get_val_data_params_and_path(self, known_apps_database_enum: dict[int, str], unknown_apps_database_enum: dict[int, str]) -&gt; tuple[TestDataParams, str]:\n        assert self.val_approach == ValidationApproach.VALIDATION_DATES\n        val_data_params = TestDataParams(\n            database_filename=self.database_filename,\n            test_period=self.val_period,\n            test_tables_paths=self._get_val_tables_paths(),\n            known_apps_database_enum=known_apps_database_enum,\n            unknown_apps_database_enum=unknown_apps_database_enum,)\n        params_hash = hashlib.sha256(json.dumps(dataclasses.asdict(val_data_params), sort_keys=True).encode()).hexdigest()\n        params_hash = params_hash[:10]\n        val_data_path = os.path.join(self.data_root, \"val-data\", f\"{params_hash}_{self.random_state}\")\n        return val_data_params, val_data_path\n\n    def _get_test_data_params_and_path(self, known_apps_database_enum: dict[int, str], unknown_apps_database_enum: dict[int, str]) -&gt; tuple[TestDataParams, str]:\n        test_data_params = TestDataParams(\n            database_filename=self.database_filename,\n            test_period=self.test_period,\n            test_tables_paths=self._get_test_tables_paths(),\n            known_apps_database_enum=known_apps_database_enum,\n            unknown_apps_database_enum=unknown_apps_database_enum,)\n        params_hash = hashlib.sha256(json.dumps(dataclasses.asdict(test_data_params), sort_keys=True).encode()).hexdigest()\n        params_hash = params_hash[:10]\n        test_data_path = os.path.join(self.data_root, \"test-data\", f\"{params_hash}_{self.random_state}\")\n        return test_data_params, test_data_path\n\n    def __str__(self):\n        _process_tag = yaml.emitter.Emitter.process_tag\n        _ignore_aliases = yaml.Dumper.ignore_aliases\n        yaml.emitter.Emitter.process_tag = lambda self, *args, **kw: None\n        yaml.Dumper.ignore_aliases = lambda self, *args, **kw: True\n        s = yaml.dump(dataclasses.asdict(self), sort_keys=False)\n        yaml.emitter.Emitter.process_tag = _process_tag\n        yaml.Dumper.ignore_aliases = _ignore_aliases\n        return s\n</code></pre>"},{"location":"reference_dataset_config/#config.DatasetConfig-functions","title":"Functions","text":""},{"location":"reference_dataset_config/#config.DatasetConfig.get_flowstats_features_len","title":"get_flowstats_features_len","text":"<pre><code>get_flowstats_features_len() -&gt; int\n</code></pre> <p>Gets the number of flow statistics features.</p> Source code in <code>cesnet_datazoo\\config.py</code> <pre><code>def get_flowstats_features_len(self) -&gt; int:\n    \"\"\"Gets the number of flow statistics features.\"\"\"\n    n = 0\n    for f in self.flowstats_features:\n        if f.startswith(\"PHIST_\"):\n            n += PHIST_BIN_COUNT\n        else:\n            n += 1\n    return n\n</code></pre>"},{"location":"reference_dataset_config/#config.DatasetConfig.get_flowstats_feature_names_expanded","title":"get_flowstats_feature_names_expanded","text":"<pre><code>get_flowstats_feature_names_expanded() -&gt; list[str]\n</code></pre> <p>Gets names of flow statistics features. Packet histograms are expanded into individual features.</p> Source code in <code>cesnet_datazoo\\config.py</code> <pre><code>def get_flowstats_feature_names_expanded(self) -&gt; list[str]:\n    \"\"\"Gets names of flow statistics features. Packet histograms are expanded into individual features.\"\"\"\n    name_mapping = {\n        \"PHIST_SRC_SIZES\": [f\"PSIZE_BIN{i}\" for i in range(1, PHIST_BIN_COUNT + 1)],\n        \"PHIST_DST_SIZES\": [f\"PSIZE_BIN{i}_REV\" for i in range(1, PHIST_BIN_COUNT + 1)],\n        \"PHIST_SRC_IPT\": [f\"IPT_BIN{i}\" for i in range(1, PHIST_BIN_COUNT + 1)],\n        \"PHIST_DST_IPT\": [f\"IPT_BIN{i}_REV\" for i in range(1, PHIST_BIN_COUNT + 1)],\n        \"FLOW_ENDREASON_IDLE\": \"FEND_IDLE\",\n        \"FLOW_ENDREASON_ACTIVE\": \"FEND_ACTIVE\",\n        \"FLOW_ENDREASON_END\": \"FEND_END\",\n        \"FLOW_ENDREASON_OTHER\": \"FEND_OTHER\",\n    }\n    feature_names = []\n    for f in self.flowstats_features:\n        if f not in name_mapping:\n            f = f if not f.startswith(\"FLAG\") else \"F\" + f.lstrip(\"FLAG\")\n            f = f.upper()\n            feature_names.append(f)\n        elif isinstance(name_mapping[f], list):\n            feature_names.extend(name_mapping[f])\n        else:\n            feature_names.append(name_mapping[f])\n    assert len(feature_names) == self.get_flowstats_features_len()\n    return feature_names\n</code></pre>"},{"location":"reference_dataset_config/#config.DatasetConfig.get_ppi_feature_names","title":"get_ppi_feature_names","text":"<pre><code>get_ppi_feature_names() -&gt; list[str]\n</code></pre> <p>Gets names of flattened per-packet information (PPI) features.</p> Source code in <code>cesnet_datazoo\\config.py</code> <pre><code>def get_ppi_feature_names(self) -&gt; list[str]:\n    \"\"\"Gets names of flattened per-packet information (PPI) features.\"\"\"\n    ppi_feature_names = [f\"IPT_{i}\" for i in range(1, PPI_MAX_LEN + 1)] + \\\n                           [f\"DIR_{i}\" for i in range(1, PPI_MAX_LEN + 1)] + \\\n                           [f\"SIZE_{i}\" for i in range(1, PPI_MAX_LEN + 1)]\n    if self.use_push_flags:\n        ppi_feature_names += [f\"PUSH_{i}\" for i in range(1, PPI_MAX_LEN + 1)]\n    return ppi_feature_names\n</code></pre>"},{"location":"reference_dataset_config/#config.DatasetConfig.get_ppi_channels","title":"get_ppi_channels","text":"<pre><code>get_ppi_channels() -&gt; int\n</code></pre> <p>Gets the number of per-packet information (PPI) channels.</p> Source code in <code>cesnet_datazoo\\config.py</code> <pre><code>def get_ppi_channels(self) -&gt; int:\n    \"\"\"Gets the number of per-packet information (PPI) channels.\"\"\"\n    if self.use_push_flags:\n        return TCP_PPI_CHANNELS\n    else:\n        return UDP_PPI_CHANNELS\n</code></pre>"},{"location":"reference_dataset_config/#config.DatasetConfig.get_feature_names","title":"get_feature_names","text":"<pre><code>get_feature_names(flatten_ppi: bool = False) -&gt; list[str]\n</code></pre> <p>Gets feature names.</p> <p>Parameters:</p> Name Type Description Default <code>flatten_ppi</code> <code>bool</code> <p>Whether to flatten PPI into individual feature names or keep one <code>PPI</code> column.</p> <code>False</code> Source code in <code>cesnet_datazoo\\config.py</code> <pre><code>def get_feature_names(self, flatten_ppi: bool = False) -&gt; list[str]:\n    \"\"\"\n    Gets feature names.\n\n    Parameters:\n        flatten_ppi: Whether to flatten PPI into individual feature names or keep one `PPI` column.\n    \"\"\"\n    feature_names = self.get_ppi_feature_names() if flatten_ppi else [\"PPI\"]\n    feature_names += self.get_flowstats_feature_names_expanded()\n    return feature_names\n</code></pre>"},{"location":"reference_dataset_config/#enums-for-configuration","title":"Enums for configuration","text":"<p>The following enums are used for dataset configuration.</p>"},{"location":"reference_dataset_config/#config.ValidationApproach","title":"config.ValidationApproach","text":"<p>The validation approach defines which samples should be used for creating a validation set.</p> SPLIT_FROM_TRAIN <code>class-attribute</code> <code>instance-attribute</code> <pre><code>SPLIT_FROM_TRAIN = 'split-from-train'\n</code></pre> <p>Split train data into train and validation. Scikit-learn <code>train_test_split</code> is used to create a random stratified validation set. The fraction of validation samples is defined in <code>train_val_split_fraction</code>.</p> VALIDATION_DATES <code>class-attribute</code> <code>instance-attribute</code> <pre><code>VALIDATION_DATES = 'validation-dates'\n</code></pre> <p>Use separate validation dates to create a validation set. Validation dates need to be specified in <code>val_dates</code>, and the name of the validation period in <code>val_period</code>.</p> NO_VALIDATION <code>class-attribute</code> <code>instance-attribute</code> <pre><code>NO_VALIDATION = 'no-validation'\n</code></pre> <p>Do not use validation. The validation dataloader and dataframe will not be available.</p>"},{"location":"reference_dataset_config/#config.AppSelection","title":"config.AppSelection","text":"<p>Applications can be divided into known and unknown classes. To use a dataset in the standard closed-world setting, use <code>ALL_KNOWN</code> to select all the applications as known. Use <code>TOPX_KNOWN</code> or <code>EXPLICIT_UNKNOWN</code> for the open-world setting and evaluation of out-of-distribution or open-set recognition methods. The <code>LONGTERM_FIXED</code> is for long-term measurements when it is desired to use the same applications for multiple subsequent train and test periods.</p> ALL_KNOWN <code>class-attribute</code> <code>instance-attribute</code> <pre><code>ALL_KNOWN = 'all-known'\n</code></pre> <p>Use all applications as known.</p> TOPX_KNOWN <code>class-attribute</code> <code>instance-attribute</code> <pre><code>TOPX_KNOWN = 'topx-known'\n</code></pre> <p>Use the first X (<code>apps_selection_topx</code>) most frequent (with the most samples) applications as known, and the rest as unknown.</p> EXPLICIT_UNKNOWN <code>class-attribute</code> <code>instance-attribute</code> <pre><code>EXPLICIT_UNKNOWN = 'explicit-unknown'\n</code></pre> <p>Use the provided list of applications (<code>apps_selection_explicit_unknown</code>) as unknown, and the rest as known.</p> LONGTERM_FIXED <code>class-attribute</code> <code>instance-attribute</code> <pre><code>LONGTERM_FIXED = 'longterm-fixed'\n</code></pre> <p>Use fixed application selection. Provide a tuple of <code>(known_apps_database_enum, unknown_apps_database_enum)</code> in <code>apps_selection_fixed_longterm</code>.</p>"},{"location":"reference_dataset_config/#config.MinTrainSamplesCheck","title":"config.MinTrainSamplesCheck","text":"<p>Depending on the selected train dates, there might be applications with not enough samples for training (what is not enough will depend on the selected classification model). The threshold for the minimum number of samples can be set with <code>min_train_samples_per_app</code>, and its default value is 100. With the <code>DISABLE_APPS</code> approach, these applications will be disabled and not used for training or testing. With the <code>WARN_AND_EXIT</code> approach, the script will print a warning and exit if applications with not enough samples are encountered. To disable this check, set <code>min_train_samples_per_app</code> to 0.</p> WARN_AND_EXIT <code>class-attribute</code> <code>instance-attribute</code> <pre><code>WARN_AND_EXIT = 'warn-and-exit'\n</code></pre> <p>Warn and exit if there are not enough training samples for some applications. It is up to the user to manually add these applications to <code>disabled_apps</code>.</p> DISABLE_APPS <code>class-attribute</code> <code>instance-attribute</code> <pre><code>DISABLE_APPS = 'disable-apps'\n</code></pre> <p>Disable applications with not enough training samples.</p>"},{"location":"reference_dataset_config/#config.DataLoaderOrder","title":"config.DataLoaderOrder","text":"<p>Validation and test sets are always loaded in sequential order \u2014 sequential meaning in the order of dates and time. However, for the train set, it is sometimes required to iterate it in random order (for example, for training a neural network). Thus, use <code>RANDOM</code> if your classification model requires it; <code>SEQUENTIAL</code> otherwise. This setting affects only train_dataloader. Dataframe get_train_df is always created in sequential order.</p> RANDOM <code>class-attribute</code> <code>instance-attribute</code> <pre><code>RANDOM = 'random'\n</code></pre> <p>Iterate train data in random order.</p> SEQUENTIAL <code>class-attribute</code> <code>instance-attribute</code> <pre><code>SEQUENTIAL = 'sequential'\n</code></pre> <p>Iterate train data in sequential (datetime) order.</p>"},{"location":"reference_datasets/","title":"Dataset classes","text":"<p>These are subclasses of <code>CesnetDataset</code> representing individual datasets available in <code>cesnet-datazoo</code>.</p>"},{"location":"reference_datasets/#datasets.datasets.CESNET_TLS22","title":"datasets.datasets.CESNET_TLS22","text":"<p>             Bases: <code>CesnetDataset</code></p> <p>Dataset class for CESNET-TLS22.</p> Source code in <code>cesnet_datazoo\\datasets\\datasets.py</code> <pre><code>class CESNET_TLS22(CesnetDataset):\n    \"\"\"Dataset class for [CESNET-TLS22][cesnet-tls22].\"\"\"\n    name = \"CESNET-TLS22\"\n    database_filename = \"CESNET-TLS22.h5\"\n    bucket_url = \"https://liberouter.org/datazoo/download?bucket=cesnet-tls22\"\n    time_periods = {\n        \"W-2021-40\": [\"20211004\", \"20211005\", \"20211006\", \"20211007\", \"20211008\", \"20211009\", \"20211010\"],\n        \"W-2021-41\": [\"20211011\", \"20211012\", \"20211013\", \"20211014\", \"20211015\", \"20211016\", \"20211017\"],\n    }\n    default_train_period = \"W-2021-40\"\n    default_test_period = \"W-2021-41\"\n</code></pre>"},{"location":"reference_datasets/#datasets.datasets.CESNET_QUIC22","title":"datasets.datasets.CESNET_QUIC22","text":"<p>             Bases: <code>CesnetDataset</code></p> <p>Dataset class for CESNET-QUIC22.</p> Source code in <code>cesnet_datazoo\\datasets\\datasets.py</code> <pre><code>class CESNET_QUIC22(CesnetDataset):\n    \"\"\"Dataset class for [CESNET-QUIC22][cesnet-quic22].\"\"\"\n    name = \"CESNET-QUIC22\"\n    database_filename = \"CESNET-QUIC22.h5\"\n    bucket_url = \"https://liberouter.org/datazoo/download?bucket=cesnet-quic22\"\n    time_periods = {\n        \"W-2022-44\": [\"20221031\", \"20221101\", \"20221102\", \"20221103\", \"20221104\", \"20221105\", \"20221106\"],\n        \"W-2022-45\": [\"20221107\", \"20221108\", \"20221109\", \"20221110\", \"20221111\", \"20221112\", \"20221113\"],\n        \"W-2022-46\": [\"20221114\", \"20221115\", \"20221116\", \"20221117\", \"20221118\", \"20221119\", \"20221120\"],\n        \"W-2022-47\": [\"20221121\", \"20221122\", \"20221123\", \"20221124\", \"20221125\", \"20221126\", \"20221127\"],\n        \"W45-47\": [\"20221107\", \"20221108\", \"20221109\", \"20221110\", \"20221111\", \"20221112\", \"20221113\",\n                        \"20221114\", \"20221115\", \"20221116\", \"20221117\", \"20221118\", \"20221119\", \"20221120\",\n                        \"20221121\", \"20221122\", \"20221123\", \"20221124\", \"20221125\", \"20221126\", \"20221127\"],\n    }\n    default_train_period = \"W-2022-44\"\n    default_test_period = \"W-2022-45\"\n</code></pre>"},{"location":"reference_datasets/#datasets.datasets.CESNET_TLS_Year22","title":"datasets.datasets.CESNET_TLS_Year22","text":"<p>             Bases: <code>CesnetDataset</code></p> <p>Dataset class for CESNET-TLS-Year22.</p> Source code in <code>cesnet_datazoo\\datasets\\datasets.py</code> <pre><code>class CESNET_TLS_Year22(CesnetDataset):\n    \"\"\"Dataset class for [CESNET-TLS-Year22][cesnet-tls-year22].\"\"\"\n    name = \"CESNET-TLS-Year22\"\n    database_filename = \"CESNET-TLS-Year22.h5\"\n    bucket_url = \"https://liberouter.org/datazoo/download?bucket=cesnet-tls-year22\"\n    time_periods = {f\"W-2022-{week}\": [] for week in range(1, 53)} | {f\"M-2022-{month}\": [] for month in range(1, 13)}\n    time_periods_gen = True\n    default_train_period = \"M-2022-9\"\n    default_test_period = \"M-2022-10\"\n</code></pre>"}]}